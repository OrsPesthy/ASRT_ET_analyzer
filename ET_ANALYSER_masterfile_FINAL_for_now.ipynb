{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c71617",
   "metadata": {},
   "source": [
    "# Eye-tracker data preprocessing <br>\n",
    "<b>Input</b>: subject-by-subject files with eye position coordinates as provided by the PsychoPy ASRT code; settings file. <br>\n",
    "    \n",
    "<b>Output </b>: trial-by-trial data, with RTs&anticipatory eye movements; data quality check; interference files. Important: currently, output files are comma-delimited, with period decimals. If you use Hungarian csv reader (excel), you will need to change the periods to commas manually at the end.\n",
    "\n",
    "<br>\n",
    "\n",
    "perks: a) it can handle missing data; b) you don't have to run it all over again if it gives you an error at any point (you just read the data that is needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427937c0",
   "metadata": {},
   "source": [
    "<b>Instruction: </b> you will need to provide inputs and follow some intstructions in the beginning! Please pay attention, I will let you know when you can move to running cells mindlessly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57932d3-240a-43ba-983a-a57e037eff40",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2c82287b",
   "metadata": {
    "code_folding": [
     15,
     19,
     29,
     35,
     56,
     91,
     102,
     145,
     238,
     301,
     372,
     434,
     494,
     520,
     579,
     604,
     655,
     737,
     798,
     838,
     870,
     946,
     997,
     1029,
     1101,
     1172,
     1192,
     1216,
     1287,
     1318,
     1337,
     1354,
     1377,
     1394,
     1440,
     1504
    ]
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#general functions\n",
    "\n",
    "def strToFloat(data):\n",
    "    \"\"\" Creates float of string values. If there are any commas as decimals, it exchanges them to period \"\"\"\n",
    "    return float(str(data).replace(\",\", \".\"))\n",
    "\n",
    "def calcRMS(values):\n",
    "    \"\"\" Calculates root mean squared\"\"\"\n",
    "    square = 0.0\n",
    "    for i in range(len(values)):\n",
    "        square += pow(values[i], 2)\n",
    "\n",
    "    mean = square / float(len(values))\n",
    "\n",
    "    return math.sqrt(mean)\n",
    "\n",
    "def convertToAngle(value_cm):\n",
    "    # This eye distance is the optimal eye-distance for Tobii Pro X3â€“120 eye-tracking distance.\n",
    "    # We use this optimal value as an approximation of the actual screen distance.\n",
    "    eye_screen_distance_cm = 65.0\n",
    "    return math.degrees(math.atan(value_cm / eye_screen_distance_cm))\n",
    "\n",
    "def check_variables_dict(example_file_path, variables_dict):\n",
    "    # Get column names from the DataFrame\n",
    "    raw_data = pd.read_csv(example_file_path, sep='\\t', encoding=\"latin-1\", low_memory=False)\n",
    "    \n",
    "    column_names = raw_data.columns.tolist()\n",
    "    \n",
    "    #Are columns called really like that?\n",
    "    column_type_values = vars_dict['stim'], vars_dict[\"PCode\"], vars_dict[\"epoch\"], vars_dict[\"trial\"], vars_dict[\"ID\"]\n",
    "        # Check if all keys in variables_dict are present in column_names\n",
    "    missing_variables = [var for var in column_type_values if var not in column_names]\n",
    "       \n",
    "    \n",
    "    if missing_variables:\n",
    "                \n",
    "        print(\"The following variables in 'variables_dict' are not present in the DataFrame columns and will make your code fail miserably:\")\n",
    "        print(missing_variables)\n",
    "    else:\n",
    "        print(\"All variables in 'variables_dict' are present in the DataFrame columns.\")\n",
    "\n",
    "#data validator functions:\n",
    "\n",
    "def data_structure_validator(vars_dict, example_file_path):\n",
    "    \"\"\"  It checks if there are actually the nr of epochs you specified in the vars_dict. \n",
    "    Notifies you if the nr of epochs in the testfile differ from the nr of epochs you specified.\"\"\"\n",
    "    path = os.getcwd() + \"/indat\"\n",
    "    ##check if testfile is in there\n",
    "    if not os.path.exists(example_file_path):\n",
    "        print(f\"The file {example_file_path} doesn't exist, not in that folder anyways. Use another file as testfile.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Using the file {example_file_path}\")\n",
    "        \n",
    "    #read test data\n",
    "    data_table = pd.read_csv(example_file_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    #check if nr of epochs match the expectations:\n",
    "    if vars_dict[\"epochN\"] != len(data_table[\"epoch\"].unique()):\n",
    "        print(f'You said there are {vars_dict[\"epochN\"]} epochs, but there are {len(data_table[\"epoch\"].unique())}, you liar!')\n",
    "    else:\n",
    "        print(\"All good, your nr of epochs match with what you specified in the vars_dict.\")\n",
    "        \n",
    "    #are there really that many trials/block?\n",
    "    if vars_dict[\"randoms\"] + vars_dict[\"trialN\"] != len(data_table[vars_dict[\"trial\"]].unique()):\n",
    "        print(\"Number of trials / epoch does not match with what you said it should be. Check if the vars_dict is wrong or your data.\")\n",
    "    else:\n",
    "        print(\"All good, the specified nr of random and sequential trials match the data structure.\")\n",
    "    \n",
    "    #is there a different PCode for IF and OS epochs?\n",
    "    if vars_dict[\"IFepoch\"] != None:\n",
    "        OSdata = data_table.loc[data_table[vars_dict[\"epoch\"]] == vars_dict[\"OSepoch\"]]\n",
    "        IFdata = data_table.loc[data_table[vars_dict[\"epoch\"]] == vars_dict[\"IFepoch\"]]\n",
    "        if OSdata[vars_dict[\"PCode\"]].unique() == IFdata[vars_dict[\"PCode\"]].unique():\n",
    "            print(\"I found the same PCode in the epochs you specified as original sequence and interference sequence epochs. U sure you typed it right?\")\n",
    "        else:\n",
    "            print(\"Aaand yepp, the interference and original sequence epochs indeed differ. Good. You are all set.\")\n",
    "\n",
    "def file_validator(indat_path):\n",
    "    \"\"\"Checks if all files in your indat folder are txt files. Notifies you if not\"\"\"\n",
    "    all_txt = True\n",
    "    for root, dirs, files in os.walk(indat_path):\n",
    "        for subject_file in files:\n",
    "            if \".txt\" not in subject_file:\n",
    "                print(f\"{subject_file} has invalid data format (not a .txt). Remove it from the 'indat' folder.\")\n",
    "                all_txt = False\n",
    "    if all_txt:\n",
    "        print(\"All good, all file formats are valid in the 'indat' folder.\")\n",
    "\n",
    "def subject_dropper(indat_path, subs_with_bad_data, vars_dict):\n",
    "    \"\"\" It checks if all subjects in the indat folder have the right amount of epochs. \n",
    "    The files with insufficient nr of epochs will be moved to a newly created junk folder\"\"\"\n",
    "    # Loop through files in the folder\n",
    "    sub_with_bad_epochN = False\n",
    "    for file in os.listdir(indat_path):\n",
    "        file_path = os.path.join(indat_path, file)\n",
    "    \n",
    "        # Ensure we are working with a file (not a folder)\n",
    "        if os.path.isfile(file_path):  \n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path, sep='\\t', usecols=['subject_number', 'epoch'], encoding=\"latin-1\", low_memory=False)\n",
    "                \n",
    "                # Extract subject number from filename\n",
    "                segments = file.split(\"_\")\n",
    "                if len(segments) > 1:\n",
    "                    sub_number = int(segments[1])  # Convert to integer\n",
    "                    # Check if number of epochs is incorrect\n",
    "                    if len(df[\"epoch\"].unique()) != vars_dict[\"epochN\"]:\n",
    "                        print(f'File {file} has {len(df[\"epoch\"].unique())} epochs insted of {vars_dict[\"epochN\"]}. Marking subject {sub_number} as bad data.')\n",
    "                        subs_with_bad_data.add(sub_number)  # Add subject to bad list\n",
    "                        sub_with_bad_epochN =True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "    if not sub_with_bad_epochN:\n",
    "        print(\"All good, all subjects seem to have sufficient amount of epochs\")\n",
    "    \n",
    "    # Move bad files\n",
    "    print(f\"bad sub list is: {subs_with_bad_data}\")\n",
    "    for file in os.listdir(indat_path):\n",
    "        segments = file.split(\"_\")\n",
    "        if len(segments) > 1:\n",
    "            try:\n",
    "                sub_number = int(segments[1])\n",
    "                if sub_number in subs_with_bad_data:\n",
    "                    source = os.path.join(indat_path, file)\n",
    "                    destination = os.path.join(junk_folder, file)\n",
    "                    shutil.move(source, destination)\n",
    "                    print(f\"Moved {file} to junk folder.\")\n",
    "            except ValueError:\n",
    "                print(f\"Skipping {file}: Cannot extract subject number.\")\n",
    "        \n",
    "def test_gaze_coordinate_coding(indat):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes the first .txt file in the specified folder to compute mean gaze coordinates for each stimulus.\n",
    "    \n",
    "    Steps:\n",
    "    1. Reads the first .txt file as a dataframe.\n",
    "    2. Converts comma decimal separators to periods and ensures numeric values.\n",
    "    3. Filters to keep only the last occurrence of each trial.\n",
    "    4. Computes mean X and Y gaze coordinates for each stimulus.\n",
    "    5. Checks if the means align with expected coordinate ranges.\n",
    "    \n",
    "    Parameters:\n",
    "        indat_path (str): Path to the folder containing .txt data files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing mean (X, Y) coordinates for each stimulus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the first file in the folder\n",
    "    files = [f for f in os.listdir(indat_path) if f.endswith(\".txt\")]\n",
    "    if not files:\n",
    "        print(\"No text files found in the folder.\")\n",
    "        return None\n",
    "\n",
    "    file_path = os.path.join(indat_path, files[0])\n",
    "    print(f\"Processing file: {files[0]}\")\n",
    "\n",
    "    # Read the file as a dataframe\n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_cols = [\"stimulus\", \"left_gaze_data_X_ADCS\", \"left_gaze_data_Y_ADCS\"]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(\"Missing required columns in the data.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Convert gaze columns to numeric, replacing commas with periods\n",
    "    df = df.applymap(lambda x: str(x).replace(\",\", \".\") if isinstance(x, str) else x)\n",
    "    df[\"left_gaze_data_X_ADCS\"] = pd.to_numeric(df[\"left_gaze_data_X_ADCS\"], errors=\"coerce\")\n",
    "    df[\"left_gaze_data_Y_ADCS\"] = pd.to_numeric(df[\"left_gaze_data_Y_ADCS\"], errors=\"coerce\")\n",
    "\n",
    "    # Dictionary to store the mean coordinates for each stimulus\n",
    "    stim_means = {}\n",
    "\n",
    "\n",
    "    # Loop through unique stimulus values\n",
    "    for stim in df['stimulus'].unique():\n",
    "        # get data where that stimulus was presented\n",
    "        stim_df = df[df[\"stimulus\"] == stim]\n",
    "        #find last lines in each trial (this is when the stimulus disappeared so the eye coordinates oughta align with the stimulus present, otherwise it wouldn't have disappeared)\n",
    "        stim_df = stim_df.loc[stim_df.groupby(\"trial\").tail(1).index]\n",
    "\n",
    "        #calc mean for both eye coordinates (using the left gaze because I felt like using the left, no political implications intended)\n",
    "        x_mean = stim_df[\"left_gaze_data_X_ADCS\"].mean(skipna=True)\n",
    "        y_mean = stim_df[\"left_gaze_data_Y_ADCS\"].mean(skipna=True)\n",
    "\n",
    "        #save it in the dictionary\n",
    "        stim_means[stim] = (round(x_mean, 2), round(y_mean, 2))\n",
    "\n",
    "\n",
    "    # Check expectations - Change it if your coding differed!!\n",
    "    incorrect_coords = {}\n",
    "    expected_coords = {\n",
    "        1: (lambda x, y: x <= 0.5 and y <= 0.5),\n",
    "        2: (lambda x, y: x >= 0.5 and y <= 0.5),\n",
    "        3: (lambda x, y: x <= 0.5 and y >= 0.5),\n",
    "        4: (lambda x, y: x >= 0.5 and y >= 0.5),\n",
    "    }\n",
    "\n",
    "    ## loop through all 4 stim locations, if the measured eye coordinates don't match the expected ones, save them as incorrect\n",
    "    for stim, (x_mean, y_mean) in stim_means.items():\n",
    "        if stim in expected_coords and not expected_coords[stim](x_mean, y_mean):\n",
    "            incorrect_coords[stim] = (x_mean, y_mean)\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    if incorrect_coords:\n",
    "        print(\"Stimuli with incorrect average coordinates:\")\n",
    "        for stim, coords in incorrect_coords.items():\n",
    "            print(f\"Stimulus {stim}: X_mean={coords[0]:.3f}, Y_mean={coords[1]:.3f}\")\n",
    "        print(\"You gotta change your getAOI function, as it expects different coordinates. Change it so it matches the coordinates in your data:\")\n",
    "        print(stim_means)\n",
    "    else:\n",
    "        print(\"All stimuli met expectations, your getAOI function functions.\")\n",
    "    \n",
    "    return stim_means   \n",
    "\n",
    "\n",
    "#data quality calculator functions:\n",
    "\n",
    "def computeMissingDataRatio(input_dir, output_file, vars_dict):\n",
    "    \"\"\"\n",
    "    Computes the missing data ratio for each subject in a given directory and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        The directory containing input .txt files for each subject.\n",
    "    output_file : str\n",
    "        The path where the computed missing data ratios will be saved as a .csv file.\n",
    "    vars_dict : dict\n",
    "        A dictionary containing necessary variable definitions:\n",
    "        - \"randoms\": Number of preparatory trials to be ignored.\n",
    "        - \"epochN\": Total number of epochs to process.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function writes the missing data ratio results to `output_file` in tab-separated format.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    - Iterates over all files in `input_dir`.\n",
    "    - Extracts subject ID from filenames.\n",
    "    - Calls `computeMissingDataRatioImpl` to compute missing data percentages.\n",
    "    - Stores results in a pandas DataFrame and exports it as a .csv file.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function expects filenames to follow a format where the subject ID is the second element when split by `_`.\n",
    "    - Non-`.txt` files are ignored with a warning message.\n",
    "    - TODO: Improve subject/epoch formatting and change list-based results to a structured DataFrame or dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    epochN = vars_dict[\"epochN\"]\n",
    "\n",
    "    missing_data_ratios = []\n",
    "    subject_epochs = []\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            if \".txt\" not in subject_file:\n",
    "                print(f'{subject_file} has invalid data format (not a .txt)')\n",
    "            else:\n",
    "                subject = subject_file.split('_')[1]\n",
    "\n",
    "                print(\"Compute missing data ratio for subject: \" + subject)\n",
    "                input_file = os.path.join(root, subject_file)\n",
    "\n",
    "                for i in range(1, epochN + 1):\n",
    "                    # TODO: Improve readability by storing epoch data in a separate column\n",
    "                    subject_epochs.append(f\"subject_{subject}_{i}\")\n",
    "\n",
    "                result = computeMissingDataRatioImpl(input_file, blockprepN, epochN)\n",
    "                # TODO: Store results in a dictionary or DataFrame for better structure\n",
    "                missing_data_ratios += result\n",
    "        break  # Process only the first directory found\n",
    "\n",
    "    missing_data = pd.DataFrame({'epoch': subject_epochs, 'missing_data_percent': missing_data_ratios})\n",
    "    missing_data.to_csv(output_file, index=False)\n",
    "\n",
    "def computeMissingDataRatioImpl(input, blockprepN, epochN):\n",
    "    \"\"\"\n",
    "    Computes the percentage of missing eye-tracking data per epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - input (str): Path to the input CSV file containing eye-tracking data.\n",
    "    - blockprepN (int): The number of preparatory trials to exclude from analysis.\n",
    "    - epochN (int): The total number of epochs in the experiment.\n",
    "\n",
    "    Returns:\n",
    "    - epoch_summary (list of str): A list where each index corresponds to an epoch (0-based),\n",
    "      containing the percentage of missing eye-tracking data as a string.\n",
    "\n",
    "    Notes:\n",
    "    - An epoch's missing data ratio is calculated as:\n",
    "        (number of missing samples in the epoch / total samples in the epoch) * 100\n",
    "    - Preparatory trials (`trial <= blockprepN`) are ignored.\n",
    "    - Calibration validation blocks (`block == '0'`) are also ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = ['block', 'trial', 'epoch', 'left_gaze_validity', 'right_gaze_validity']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(input, sep='\\t', encoding=\"latin1\", usecols=required_cols)\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "    epoch_column = data_table[\"epoch\"]\n",
    "    left_gaze_validity = data_table[\"left_gaze_validity\"]\n",
    "    right_gaze_validity = data_table[\"right_gaze_validity\"]\n",
    "\n",
    "    epoch_all_data = {}\n",
    "    epoch_missing_data = {}\n",
    "    for i in range(len(trial_column)):\n",
    "        # We ignore preparatory trials.\n",
    "        if int(trial_column[i]) <= blockprepN:\n",
    "            continue\n",
    "\n",
    "        # We ignore calibration validation blocks.\n",
    "        if str(block_column[i]) == '0':\n",
    "            continue\n",
    "        \n",
    "        current_epoch = int(epoch_column[i])\n",
    "\n",
    "        # We count all eye-tracking samples for the current epoch.\n",
    "        # can we change floats to int, there are no fraction of samples\n",
    "\n",
    "        epoch_all_data[current_epoch] = epoch_all_data.get(current_epoch, 0) + 1\n",
    "\n",
    "        # We count all missing eye-tracking data for the current epoch.\n",
    "        if not bool(left_gaze_validity[i]) and not bool(right_gaze_validity[i]):\n",
    "            if current_epoch in epoch_missing_data.keys():\n",
    "                epoch_missing_data[current_epoch] += 1\n",
    "            else:\n",
    "                epoch_missing_data[current_epoch] = 1\n",
    "        \n",
    "    # We compute missing data ratio for all epochs.\n",
    "    epoch_summary = np.zeros(epochN).tolist()\n",
    "    \n",
    "    for epoch in epoch_all_data.keys():\n",
    "        epoch_summary[epoch - 1] = (epoch_missing_data[epoch] / epoch_all_data[epoch]) * 100.0\n",
    "\n",
    "    return epoch_summary\n",
    "\n",
    "def computeMissingDataRatio(input_dir, output_file, vars_dict):\n",
    "    \"\"\"\n",
    "    Computes the missing data ratio for each subject in a given directory and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        The directory containing input .txt files for each subject.\n",
    "    output_file : str\n",
    "        The path where the computed missing data ratios will be saved as a .csv file.\n",
    "    vars_dict : dict\n",
    "        A dictionary containing necessary variable definitions:\n",
    "        - \"randoms\": Number of preparatory trials to be ignored.\n",
    "        - \"epochN\": Total number of epochs to process.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function writes the missing data ratio results to `output_file` in tab-separated format.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    - Iterates over all files in `input_dir`.\n",
    "    - Extracts subject ID from filenames.\n",
    "    - Calls `computeMissingDataRatioImpl` to compute missing data percentages.\n",
    "    - Stores results in a pandas DataFrame and exports it as a .csv file.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function expects filenames to follow a format where the subject ID is the second element when split by `_`.\n",
    "    - Non-`.txt` files are ignored with a warning message.\n",
    "    - TODO: Improve subject/epoch formatting and change list-based results to a structured DataFrame or dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    epochN = vars_dict[\"epochN\"]\n",
    "\n",
    "    missing_data_ratios = []\n",
    "    subject_epochs = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            if \".txt\" not in subject_file:\n",
    "                print(f'{subject_file} has invalid data format (not a .txt)')\n",
    "            else:\n",
    "                subject = subject_file.split('_')[1]\n",
    "\n",
    "                print(\"Compute missing data ratio for subject: \" + subject)\n",
    "                input_file = os.path.join(root, subject_file)\n",
    "\n",
    "                for i in range(1,epochN + 1):\n",
    "                    #todo: make it more userfriendly (epoch in sep column from sub)\n",
    "                    subject_epochs.append(f\"subject_{subject}_{i}\")\n",
    "\n",
    "                result = computeMissingDataRatioImpl(input_file, blockprepN, epochN)\n",
    "                # dafuck is this, todo change it to dataframe cols / dictionary or something\n",
    "                missing_data_ratios += result\n",
    "        break\n",
    "\n",
    "    missing_data = pd.DataFrame({'epoch' : subject_epochs, 'missing_data_percent' : missing_data_ratios})\n",
    "    missing_data.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "def computeDistanceImpl(input, blockprepN, epochN):\n",
    "\n",
    "\n",
    "    required_cols = ['block', 'trial', 'epoch', 'left_gaze_validity', 'right_gaze_validity',\n",
    "                     'left_eye_distance', 'right_eye_distance']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(input, sep='\\t', encoding=\"latin1\", usecols=required_cols, low_memory = False)\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for participant {participant_id}. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    epoch_column = data_table[\"epoch\"]\n",
    "    left_gaze_validity = data_table[\"left_gaze_validity\"]\n",
    "    right_gaze_validity = data_table[\"right_gaze_validity\"]\n",
    "    left_eye_distance = data_table[\"left_eye_distance\"]\n",
    "    right_eye_distance = data_table[\"right_eye_distance\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "\n",
    "    epoch_distances = {}\n",
    "    for i in range(len(trial_column)):\n",
    "        # We ignore preparatory trials.\n",
    "        if int(trial_column[i]) <= blockprepN:\n",
    "            continue\n",
    "\n",
    "        # We ignore calibration validation blocks.\n",
    "        if str(block_column[i]) == '0':\n",
    "            continue\n",
    "\n",
    "        # Use the distance from the valid eye data. If there is two, we use average.\n",
    "        distance_mm = -1.0\n",
    "        if bool(left_gaze_validity[i]) and bool(right_gaze_validity[i]):\n",
    "            distance_mm = (strToFloat(left_eye_distance[i]) + strToFloat(right_eye_distance[i])) / 2.0\n",
    "        elif bool(left_gaze_validity[i]):\n",
    "            distance_mm = strToFloat(left_eye_distance[i])\n",
    "        elif bool(right_gaze_validity[i]):\n",
    "            distance_mm = strToFloat(right_eye_distance[i])\n",
    "\n",
    "        # Collect all distances of all epochs,\n",
    "        if distance_mm > 0.0:\n",
    "            distance_cm = distance_mm / 10.0\n",
    "            current_epoch = int(epoch_column[i])\n",
    "            if current_epoch in epoch_distances.keys():\n",
    "                epoch_distances[current_epoch].append(distance_cm)\n",
    "            else:\n",
    "                epoch_distances[current_epoch] = [distance_cm]\n",
    "\n",
    "    # Compute median distance of subject eyes for all epochs.\n",
    "    epoch_summary = np.zeros(epochN).tolist()\n",
    "    for epoch in epoch_distances.keys():\n",
    "        epoch_summary[epoch - 1] = str(np.median(epoch_distances[epoch]))\n",
    "\n",
    "\n",
    "    return epoch_summary\n",
    "\n",
    "def computeDistance(input_dir, output_file, vars_dict):\n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    epochN = vars_dict[\"epochN\"]\n",
    "\n",
    "    median_distances = []\n",
    "    subject_epochs = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            if \".txt\" not in subject_file:\n",
    "                print(f'{subject_file} has invalid data format (not a .txt)')\n",
    "            else:\n",
    "                subject = subject_file.split('_')[1]\n",
    "\n",
    "                print(\"Compute eye-screen distance data for subject: \" + subject)\n",
    "                input_file = os.path.join(root, subject_file)\n",
    "\n",
    "                for i in range(1,epochN+1):\n",
    "                    subject_epochs.append(\"subject_\" + subject + \"_\" + str(i))\n",
    "\n",
    "                epoch_medians = computeDistanceImpl(input_file, blockprepN, epochN)\n",
    "                median_distances += epoch_medians\n",
    "        break\n",
    "\n",
    "    distance_data = pd.DataFrame({'epoch' : subject_epochs, 'median_distance_cm' : median_distances})\n",
    "    distance_data.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "def computeDistanceImpl(input, blockprepN, epochN):\n",
    "    \"\"\" Computes screen-to-eye distance for each participant in the indat folder. \"\"\"\n",
    "    \n",
    "    required_cols = ['block', 'trial', 'epoch', 'left_gaze_validity', 'right_gaze_validity',\n",
    "                                          'left_eye_distance', 'right_eye_distance']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(input, sep='\\t', encoding=\"latin1\", usecols=required_cols)\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    epoch_column = data_table[\"epoch\"]\n",
    "    left_gaze_validity = data_table[\"left_gaze_validity\"]\n",
    "    right_gaze_validity = data_table[\"right_gaze_validity\"]\n",
    "    left_eye_distance = data_table[\"left_eye_distance\"]\n",
    "    right_eye_distance = data_table[\"right_eye_distance\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "\n",
    "    epoch_distances = {}\n",
    "    for i in range(len(trial_column)):\n",
    "        # We ignore preparatory trials.\n",
    "        if int(trial_column[i]) <= blockprepN:\n",
    "            continue\n",
    "\n",
    "        # We ignore calibration validation blocks.\n",
    "        if str(block_column[i]) == '0':\n",
    "            continue\n",
    "\n",
    "        # Use the distance from the valid eye data. If there is two, we use average.\n",
    "        distance_mm = -1.0\n",
    "        if bool(left_gaze_validity[i]) and bool(right_gaze_validity[i]):\n",
    "            distance_mm = (strToFloat(left_eye_distance[i]) + strToFloat(right_eye_distance[i])) / 2.0\n",
    "        elif bool(left_gaze_validity[i]):\n",
    "            distance_mm = strToFloat(left_eye_distance[i])\n",
    "        elif bool(right_gaze_validity[i]):\n",
    "            distance_mm = strToFloat(right_eye_distance[i])\n",
    "\n",
    "        # Collect all distances of all epochs,\n",
    "        if distance_mm > 0.0:\n",
    "            distance_cm = distance_mm / 10.0\n",
    "            current_epoch = int(epoch_column[i])\n",
    "            if current_epoch in epoch_distances.keys():\n",
    "                epoch_distances[current_epoch].append(distance_cm)\n",
    "            else:\n",
    "                epoch_distances[current_epoch] = [distance_cm]\n",
    "\n",
    "    # Compute median distance of subject eyes for all epochs.\n",
    "    epoch_summary = np.zeros(epochN).tolist()\n",
    "    for epoch in epoch_distances.keys():\n",
    "        epoch_summary[epoch - 1] = np.median(epoch_distances[epoch])\n",
    "\n",
    "\n",
    "    return epoch_summary\n",
    "\n",
    "def computeDistance(input_dir, output_file, vars_dict):\n",
    "\n",
    "    \n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    epochN = vars_dict[\"epochN\"]\n",
    "\n",
    "    median_distances = []\n",
    "    subject_epochs = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            subject = subject_file.split('_')[1]\n",
    "\n",
    "            print(\"Compute eye-screen distance data for subject: \" + subject)\n",
    "            input_file = os.path.join(root, subject_file)\n",
    "\n",
    "            for i in range(1,epochN+1):\n",
    "                subject_epochs.append(\"subject_\" + subject + \"_\" + str(i))\n",
    "\n",
    "            epoch_medians = computeDistanceImpl(input_file, blockprepN, epochN)\n",
    "            median_distances += epoch_medians\n",
    "        break\n",
    "\n",
    "    distance_data = pd.DataFrame({'epoch' : subject_epochs, 'median_distance_cm' : median_distances})\n",
    "    distance_data.to_csv(output_file, index=False)\n",
    "\n",
    "def calcDistancesForFixation(j, k, data_table):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between the left and right eyes for a given fixation period.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    j : int\n",
    "        The starting index of the fixation period.\n",
    "    k : int\n",
    "        The ending index of the fixation period.\n",
    "    data_table : pd.DataFrame\n",
    "        A dataframe containing gaze validity and gaze position data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of float\n",
    "        A list of eye-to-eye distances converted to angles for valid gaze data points.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    - Iterates over samples from index `j` to `k`.\n",
    "    - If both eyes provide valid gaze data, calculates Euclidean distance.\n",
    "    - Converts distances to angles using `convertToAngle()` before appending.\n",
    "    - Ignores invalid data points where either eye lacks valid gaze data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_eye_to_eye_distances = []\n",
    "    left_gaze_validity = data_table[\"left_gaze_validity\"]\n",
    "    right_gaze_validity = data_table[\"right_gaze_validity\"]\n",
    "    left_gaze_data_X_PCMCS = data_table[\"left_gaze_data_X_PCMCS\"]\n",
    "    left_gaze_data_Y_PCMCS = data_table[\"left_gaze_data_Y_PCMCS\"]\n",
    "    right_gaze_data_X_PCMCS = data_table[\"right_gaze_data_X_PCMCS\"]\n",
    "    right_gaze_data_Y_PCMCS = data_table[\"right_gaze_data_Y_PCMCS\"]\n",
    "\n",
    "    for i in range(j, k + 1):\n",
    "        eye_to_eye_distance = -1.0\n",
    "        if bool(left_gaze_validity[i]) and bool(right_gaze_validity[i]):\n",
    "            left_X = strToFloat(left_gaze_data_X_PCMCS[i])\n",
    "            left_Y = strToFloat(left_gaze_data_Y_PCMCS[i])\n",
    "            right_X = strToFloat(right_gaze_data_X_PCMCS[i])\n",
    "            right_Y = strToFloat(right_gaze_data_Y_PCMCS[i])\n",
    "            X_distance = abs(left_X - right_X)\n",
    "            Y_distance = abs(left_Y - right_Y)\n",
    "            eye_to_eye_distance = math.sqrt(pow(X_distance, 2) + pow(Y_distance, 2))\n",
    "\n",
    "        if eye_to_eye_distance > 0.0:\n",
    "            all_eye_to_eye_distances.append(convertToAngle(eye_to_eye_distance))\n",
    "\n",
    "    return all_eye_to_eye_distances\n",
    "\n",
    "def computeRMSEyeToEyeImpl(input, blockprepN, fixation_duration_threshold, epochN):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square (RMS) of eye-to-eye distances for fixations in each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input : str\n",
    "        The path to the input file containing gaze data.\n",
    "    blockprepN : int\n",
    "        The number of preparatory trials to be ignored.\n",
    "    fixation_duration_threshold : int\n",
    "        The minimum number of samples required to consider a fixation valid.\n",
    "    epochN : int\n",
    "        The total number of epochs.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of str\n",
    "        A list of median RMS values (as strings) for each epoch.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    - Reads gaze data from a tab-separated file.\n",
    "    - Iterates over trials, skipping preparatory trials and calibration blocks.\n",
    "    - Extracts the last fixation in each trial and computes distances.\n",
    "    - Calculates the RMS of the distances for each epoch.\n",
    "    - Returns the median RMS values for each epoch as a list.\n",
    "\n",
    "    \"\"\"\n",
    "    required_cols = ['block', 'trial', 'epoch', 'left_gaze_validity', 'right_gaze_validity',\n",
    "                                          'left_gaze_data_X_PCMCS', 'left_gaze_data_Y_PCMCS', 'right_gaze_data_X_PCMCS', \n",
    "                                          'right_gaze_data_Y_PCMCS']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(input, sep='\\t', encoding=\"latin1\", usecols=required_cols)\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "    epoch_column = data_table[\"epoch\"]\n",
    "\n",
    "    epoch_rmss= {}\n",
    "    for i in range(len(trial_column) - 1):\n",
    "        # We ignore preparatory trials.\n",
    "        if int(trial_column[i]) <= blockprepN:\n",
    "            continue\n",
    "\n",
    "        # We ignore calibration validation blocks.\n",
    "        if str(block_column[i]) == '0':\n",
    "            continue\n",
    "\n",
    "        # end of trial -> check samples of the last fixation (duration threshold shows the number of samples)\n",
    "        if trial_column[i] != trial_column[i + 1] or i + 1 == len(trial_column) - 1:\n",
    "            # Distance values for the fixation samples.\n",
    "            if i + 1 == len(trial_column) - 1:\n",
    "                all_distances = calcDistancesForFixation(i - fixation_duration_threshold + 2, i + 1, data_table)\n",
    "            else:\n",
    "                all_distances = calcDistancesForFixation(i - fixation_duration_threshold + 1, i, data_table)\n",
    "\n",
    "            if len(all_distances) > 0:\n",
    "                current_epoch = int(epoch_column[i])\n",
    "\n",
    "                # Calc RMS of all collected distances.\n",
    "                new_RMS = calcRMS(all_distances)\n",
    "                if current_epoch in epoch_rmss.keys():\n",
    "                    epoch_rmss[current_epoch].append(new_RMS)\n",
    "                else:\n",
    "                    epoch_rmss[current_epoch] = [new_RMS]\n",
    "\n",
    "    # We compute median RMS(E2E) for all epochs.\n",
    "    epoch_summary = np.zeros(epochN).tolist()\n",
    "    for epoch in epoch_rmss.keys():\n",
    "        epoch_summary[epoch - 1] = str(np.median(epoch_rmss[epoch]))\n",
    "\n",
    "    return epoch_summary\n",
    "\n",
    "def computeRMSEyeToEye(input_dir, output_file, vars_dict):\n",
    "    \"\"\"\n",
    "    Computes the median Root Mean Square (RMS) of eye-to-eye distances for each subject in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        The directory containing input files for each subject.\n",
    "    output_file : str\n",
    "        The path where the computed RMS values will be saved as a .csv file.\n",
    "    vars_dict : dict\n",
    "        A dictionary containing key experimental parameters:\n",
    "        - \"randoms\": Number of preparatory trials to ignore.\n",
    "        - \"epochN\": Total number of epochs.\n",
    "        - \"fixation_threshold\": Minimum fixation duration threshold.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function saves the results to `output_file` in tab-separated format.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    - Iterates through `.txt` files in `input_dir`.\n",
    "    - Extracts subject ID from filenames.\n",
    "    - Calls `computeRMSEyeToEyeImpl()` to compute RMS values.\n",
    "    - Stores results in a pandas DataFrame and exports it as a `.csv` file.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Assumes filenames follow a format where the subject ID is the second element when split by `_`.\n",
    "    - TODO: Improve subject/epoch formatting and store results in a structured DataFrame instead of a list.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    epochN = vars_dict[\"epochN\"]\n",
    "    fix_threshold = vars_dict[\"fixation_threshold\"]\n",
    "\n",
    "    median_rms = []\n",
    "    subject_epochs = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            if \".txt\" not in subject_file:\n",
    "                print(f'{subject_file} has invalid data format (not a .txt)')\n",
    "            else:\n",
    "                subject = subject_file.split('_')[1]\n",
    "\n",
    "                print(\"Compute RMS(E2E) for subject:  \" + subject)\n",
    "                input_file = os.path.join(root, subject_file)\n",
    "\n",
    "                for i in range(1,epochN+1):\n",
    "                    subject_epochs.append(\"subject_\" + subject + \"_\" + str(i))\n",
    "\n",
    "                result = computeRMSEyeToEyeImpl(input_file, blockprepN, fix_threshold, epochN)\n",
    "                median_rms += result\n",
    "        break\n",
    "\n",
    "    RMS_E2E_data = pd.DataFrame({'epoch' : subject_epochs, 'RMS(E2E)_median' : median_rms})\n",
    "    RMS_E2E_data.to_csv(output_file, index=False)\n",
    "\n",
    "def getEyePos(data_table, i):\n",
    "    \"\"\"\n",
    "    Computes the gaze position by averaging valid eye data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_table : pd.DataFrame\n",
    "        The dataframe containing gaze validity and position data.\n",
    "    i : int\n",
    "        The index of the current sample.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple or str\n",
    "        A tuple (pos_X, pos_Y) representing the computed gaze position,\n",
    "        or 'none' if no valid data is available.\n",
    "    \"\"\"\n",
    "    \n",
    "    left_gaze_validity = data_table[\"left_gaze_validity\"][i]\n",
    "    right_gaze_validity = data_table[\"right_gaze_validity\"][i]\n",
    "    left_gaze_data_X_PCMCS = data_table[\"left_gaze_data_X_PCMCS\"][i]\n",
    "    left_gaze_data_Y_PCMCS = data_table[\"left_gaze_data_Y_PCMCS\"][i]\n",
    "    right_gaze_data_X_PCMCS = data_table[\"right_gaze_data_X_PCMCS\"][i]\n",
    "    right_gaze_data_Y_PCMCS = data_table[\"right_gaze_data_Y_PCMCS\"][i]\n",
    "\n",
    "    # Calculate current eye pos based on the valid eye positions (hybrid computation).\n",
    "    if bool(left_gaze_validity) and bool(right_gaze_validity):\n",
    "        pos_X = (strToFloat(left_gaze_data_X_PCMCS) + strToFloat(right_gaze_data_X_PCMCS)) / 2.0\n",
    "        pos_Y = (strToFloat(left_gaze_data_Y_PCMCS) + strToFloat(right_gaze_data_Y_PCMCS)) / 2.0\n",
    "    elif bool(left_gaze_validity):\n",
    "        pos_X = strToFloat(left_gaze_data_X_PCMCS)\n",
    "        pos_Y = strToFloat(left_gaze_data_Y_PCMCS)\n",
    "    elif bool(right_gaze_validity):\n",
    "        pos_X = strToFloat(right_gaze_data_X_PCMCS)\n",
    "        pos_Y = strToFloat(right_gaze_data_Y_PCMCS)\n",
    "    else: # No valid data\n",
    "        return 'none'\n",
    "\n",
    "    return (pos_X, pos_Y)\n",
    "\n",
    "def calcDistancesForFixation_s2s(j, k, data_table):\n",
    "    \"\"\"\n",
    "    Computes Euclidean distances between consecutive gaze positions within a fixation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    j : int\n",
    "        The starting index of the fixation period.\n",
    "    k : int\n",
    "        The ending index of the fixation period.\n",
    "    data_table : pd.DataFrame\n",
    "        The dataframe containing gaze data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of float\n",
    "        A list of visual angle distances for valid gaze samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_distances = []\n",
    "    # Check all samples with an index within j and k.\n",
    "    for i in range(j, k):\n",
    "        prev_pos = getEyePos(data_table, i)\n",
    "        next_pos = getEyePos(data_table, i + 1)\n",
    "        if prev_pos != 'none' and next_pos != 'none':\n",
    "            # Distance in cm, based on psychopy coordinate system.\n",
    "            distance = math.sqrt(pow(prev_pos[0] - next_pos[0], 2) + pow(prev_pos[1] - next_pos[1], 2))\n",
    "            # Convert the distance in cm to a visual angle. It's more common to use visual angle values.\n",
    "            all_distances.append(convertToAngle(distance))\n",
    "    \n",
    "    return all_distances\n",
    "\n",
    "def computeRMSSampleToSampleImpl(input, preparatory_trial_number, fixation_duration_threshold, vars_dict):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square (RMS) of sample-to-sample distances for fixations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input : str\n",
    "        Path to the input file containing gaze data.\n",
    "    preparatory_trial_number : int\n",
    "        The number of preparatory trials to ignore.\n",
    "    fixation_duration_threshold : int\n",
    "        The minimum number of samples required for a valid fixation.\n",
    "    vars_dict : dict\n",
    "        Dictionary containing experiment parameters, including epoch count.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of str\n",
    "        A list of median RMS values for each epoch.\n",
    "    \"\"\"\n",
    "    required_cols = ['block', 'trial', 'epoch', 'left_gaze_validity', 'right_gaze_validity',\n",
    "                                          'left_gaze_data_X_PCMCS', 'left_gaze_data_Y_PCMCS', 'right_gaze_data_X_PCMCS', 'right_gaze_data_Y_PCMCS']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(input, sep='\\t', encoding=\"latin1\", usecols=required_cols)\n",
    "\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "    epoch_column = data_table[\"epoch\"]\n",
    "\n",
    "    epoch_rmss= {}\n",
    "    for i in range(len(trial_column) - 1):\n",
    "        # We ignore preparatory trials.\n",
    "        if int(trial_column[i]) <= preparatory_trial_number:\n",
    "            continue\n",
    "\n",
    "        # We ignore calibration validation blocks.\n",
    "        if str(block_column[i]) == '0':\n",
    "            continue\n",
    "\n",
    "        # end of trial -> check samples of the last fixation (duration threshold shows the number of samples)\n",
    "        if trial_column[i] != trial_column[i + 1] or i + 1 == len(trial_column) - 1:\n",
    "            # Distance values for the fixation samples.\n",
    "            if i + 1 == len(trial_column) - 1:\n",
    "                all_distances = calcDistancesForFixation_s2s(i - fixation_duration_threshold + 2, i + 1, data_table)\n",
    "            else:\n",
    "                all_distances = calcDistancesForFixation_s2s(i - fixation_duration_threshold + 1, i, data_table)\n",
    "\n",
    "            if len(all_distances) > 0:\n",
    "                current_epoch = int(epoch_column[i])\n",
    "\n",
    "                # Calc RMS of all collected distances.\n",
    "                new_RMS = calcRMS(all_distances)\n",
    "                if current_epoch in epoch_rmss.keys():\n",
    "                    epoch_rmss[current_epoch].append(new_RMS)\n",
    "                else:\n",
    "                    epoch_rmss[current_epoch] = [new_RMS]\n",
    "\n",
    "    # We compute median RMS(S2S) for all epochs.\n",
    "    epoch_summary = np.zeros(vars_dict[\"epochN\"]).tolist()\n",
    "    for epoch in epoch_rmss.keys():\n",
    "        epoch_summary[epoch - 1] = np.median(epoch_rmss[epoch])\n",
    "\n",
    "    if len(epoch_summary) != vars_dict[\"epochN\"]:\n",
    "        raise Exception(f'Error: The input data should contain exactly {vars_dict[\"epochN\"]} epochs for this data analysis.')\n",
    "\n",
    "    return epoch_summary\n",
    "\n",
    "def computeRMSSampleToSample(input_dir, output_file, vars_dict):\n",
    "    \"\"\"\n",
    "    Computes and saves the median Root Mean Square (RMS) of sample-to-sample distances for each subject.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing input files for each subject.\n",
    "    output_file : str\n",
    "        Path to save the computed RMS values in a CSV file.\n",
    "    vars_dict : dict\n",
    "        Dictionary containing experiment parameters, including:\n",
    "        - \"randoms\": Number of preparatory trials to ignore.\n",
    "        - \"epochN\": Total number of epochs.\n",
    "        - \"fixation_threshold\": Minimum fixation duration threshold.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves the results to `output_file` in tab-separated format.\n",
    "    \"\"\"\n",
    "    \n",
    "    parent_folder = os.getcwd()\n",
    "\n",
    "    median_rmss = []\n",
    "    subject_epochs = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            if \".txt\" not in subject_file:\n",
    "                print(f'{subject_file} has invalid data format (not a .txt)')\n",
    "            else:\n",
    "                subject = subject_file.split('_')[1]\n",
    "\n",
    "                print(\"Compute RMS(S2S) for subject: \" + subject)\n",
    "\n",
    "                input_file = os.path.join(root, subject_file)\n",
    "\n",
    "                for i in range(1,vars_dict[\"epochN\"] + 1):\n",
    "                    subject_epochs.append(\"subject_\" + subject + \"_\" + str(i))\n",
    "\n",
    "                RMS = computeRMSSampleToSampleImpl(input_file, vars_dict[\"randoms\"], vars_dict[\"fixation_threshold\"], vars_dict)\n",
    "                median_rmss += RMS\n",
    "        break\n",
    "\n",
    "    RMS_S2S_data = pd.DataFrame({'epoch' : subject_epochs, 'RMS(S2S)_median' : median_rmss})\n",
    "    RMS_S2S_data.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "#actual calculations of the output dataset \n",
    "\n",
    "#trialdata\n",
    "def computeTrialLevelData(input_dir, output_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes raw data files to compute trial-level reaction times (RT) and last area of interest (AOI).\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir (str): Path to the directory containing raw .txt data files.\n",
    "        output_dir (str): Path to the directory where output CSV files will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            if not subject_file.endswith(\".txt\"):\n",
    "                print(f'{subject_file} has invalid data format (not a .txt)')\n",
    "                continue\n",
    "            \n",
    "            subject = subject_file.split('_')[1]\n",
    "            print(f\"Compute trial level data for subject: {subject}\")\n",
    "            \n",
    "            raw_data_path = os.path.join(root, subject_file)\n",
    "            RT_data_path = os.path.join(output_dir, f'subject_{subject}__trial_log.csv')\n",
    "            \n",
    "            RT_data = calcRTColumn(raw_data_path)\n",
    "            last_AOI_data = calcLastAOIColumn(raw_data_path)\n",
    "            \n",
    "            if RT_data is not None and last_AOI_data is not None:\n",
    "                generateOutput(raw_data_path, RT_data_path, RT_data, last_AOI_data)\n",
    "            else:\n",
    "                print(f\"Missing data for participant #{subject}\")\n",
    "        break\n",
    "\n",
    "def calcRTColumn(raw_file_name):\n",
    "    \"\"\"\n",
    "    Computes the reaction time (RT) for each trial based on gaze timestamp data.\n",
    "    \n",
    "    Parameters:\n",
    "        raw_file_name (str): Path to the raw data file.\n",
    "    \n",
    "    Returns:\n",
    "        list: Reaction times for each trial.\n",
    "    \"\"\"\n",
    "    \n",
    "    required_cols = ['block', 'trial', 'trial_phase', 'gaze_data_time_stamp']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(raw_file_name, sep='\\t', encoding=\"latin1\", \n",
    "                                 usecols=required_cols, \n",
    "                                 dtype={'gaze_data_time_stamp': 'float64'})\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant in calcRTcol. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return []  # Return None to indicate failure, allowing the loop to continue\n",
    "       \n",
    "    \n",
    "\n",
    "    RT_data = []\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "    trial_phase_column = data_table[\"trial_phase\"]\n",
    "    time_stamp_column = data_table[\"gaze_data_time_stamp\"]\n",
    "\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    start_time_found = False\n",
    "    end_time_found = False\n",
    "\n",
    "    for i in range(len(trial_column)):\n",
    "\n",
    "        # we reached the next trial's first data (we compute the previous trial's reaction time).\n",
    "        if i == 0:\n",
    "            reached_next_trial = False\n",
    "        else:\n",
    "            reached_next_trial = trial_column[i] != trial_column[i - 1]\n",
    "        if reached_next_trial:\n",
    "            # we don't compute RT for 0 indexed blocks, which are calibration validation blocks.\n",
    "            if str(block_column[i - 1]) != \"0\":\n",
    "                RT_data.append(calcRTTrial(start_time_found, start_time, end_time_found, end_time, int(time_stamp_column[i - 1])))\n",
    "\n",
    "            start_time_found = False\n",
    "            end_time_found = False\n",
    "\n",
    "        # stimulus appears on the screen -> start time\n",
    "        if trial_phase_column[i] == \"stimulus_on_screen\" and not start_time_found:\n",
    "            start_time = int(time_stamp_column[i])\n",
    "            start_time_found = True\n",
    "\n",
    "        # stimulus disappears from the screen -> end time\n",
    "        if trial_phase_column[i] == \"after_reaction\" and not end_time_found:\n",
    "            end_time = int(time_stamp_column[i - 1])\n",
    "            end_time_found = True\n",
    "            #Do not panic, if the endtime is not found here, we'll just save the last time stamp of the given trial - see calcRTTrial funct\n",
    "\n",
    "        # we need to handle the last trial differently at the end of the data file.\n",
    "        reached_end_of_file = (i == len(trial_column) - 1)\n",
    "        if reached_end_of_file:\n",
    "            # we don't compute RT for 0 indexed blocks, which are calibration validation blocks.\n",
    "            if str(block_column[i - 1]) != \"0\":\n",
    "                RT_data.append(calcRTTrial(start_time_found, start_time, end_time_found, end_time, int(time_stamp_column[i])))\n",
    "        \n",
    "    return RT_data\n",
    "\n",
    "def calcLastAOIColumn(raw_file_name):\n",
    "    \"\"\"\n",
    "    Determines the last area of interest (AOI) before the stimulus appears.\n",
    "    \n",
    "    Parameters:\n",
    "        raw_file_name (str): Path to the raw data file.\n",
    "    \n",
    "    Returns:\n",
    "        list: Last AOI values for each trial.\n",
    "    \"\"\"\n",
    "    \n",
    "    required_cols = ['block', 'trial', 'trial_phase', 'left_gaze_validity', 'right_gaze_validity',\n",
    "                                         'left_gaze_data_X_ADCS', 'left_gaze_data_Y_ADCS', 'right_gaze_data_X_ADCS', 'right_gaze_data_Y_ADCS']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        data_table = pd.read_csv(raw_file_name, sep='\\t', encoding=\"latin1\", usecols=required_cols)\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant in lastAOI. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return []  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "    anticipation_data = []\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    block_column = data_table[\"block\"]\n",
    "    trial_phase_column = data_table[\"trial_phase\"]\n",
    "    left_gaze_validity_column = data_table[\"left_gaze_validity\"]\n",
    "    right_gaze_validity_column = data_table[\"right_gaze_validity\"]\n",
    "    left_gaze_data_X_column = data_table[\"left_gaze_data_X_ADCS\"]\n",
    "    left_gaze_data_Y_column = data_table[\"left_gaze_data_Y_ADCS\"]\n",
    "    right_gaze_data_X_column = data_table[\"right_gaze_data_X_ADCS\"]\n",
    "    right_gaze_data_Y_column = data_table[\"right_gaze_data_Y_ADCS\"]\n",
    "\n",
    "    last_AOI = -1\n",
    "    for i in range(len(trial_column)):\n",
    "        # we reached the next trial's first data (we compute the previous trial's last visited AOI).\n",
    "        if i == 0:\n",
    "            reached_next_trial = False\n",
    "        else:\n",
    "            reached_next_trial = trial_column[i] != trial_column[i - 1]\n",
    "        if reached_next_trial:\n",
    "            # we don't compute last AOI data for 0 indexed blocks, which are calibration validation blocks.\n",
    "            if str(block_column[i - 1]) != \"0\":\n",
    "                if last_AOI == -1:\n",
    "                    anticipation_data.append('none')\n",
    "                else:\n",
    "                    anticipation_data.append(last_AOI)\n",
    "            last_AOI = -1\n",
    "\n",
    "        # get AOI during RSI\n",
    "        if trial_phase_column[i] == 'before_stimulus':\n",
    "            current_AOI = getAOI(bool(left_gaze_validity_column[i]), bool(right_gaze_validity_column[i]),\n",
    "                                 strToFloat(left_gaze_data_X_column[i]), strToFloat(left_gaze_data_Y_column[i]),\n",
    "                                 strToFloat(right_gaze_data_X_column[i]), strToFloat(right_gaze_data_Y_column[i]))\n",
    "            if current_AOI != -1:\n",
    "                last_AOI = current_AOI\n",
    "\n",
    "        # we need to handle the last trial differently at the end of the data file.\n",
    "        reached_end_of_file = (i == len(trial_column) - 1)\n",
    "        if reached_end_of_file:\n",
    "            # we don't compute last AOI data for 0 indexed blocks, which are calibration validation blocks.\n",
    "            if str(block_column[i - 1]) != \"0\":\n",
    "                if last_AOI == -1:\n",
    "                    anticipation_data.append('none')\n",
    "                else:\n",
    "                    anticipation_data.append(last_AOI)\n",
    "                last_AOI = -1\n",
    "\n",
    "    return anticipation_data\n",
    "\n",
    "def calcRTTrial(start_time_found, start_time, end_time_found, end_time, last_time_stamp_of_trial):\n",
    "    \"\"\"It calculates the differences between end time and start time and converts it into msec\"\"\"\n",
    "    # We calculate the elapsed time during the stimulus was on the screen.\n",
    "    if start_time_found and end_time_found:\n",
    "        RT_ms = (end_time - start_time) / 1000.0\n",
    "        #deleted str()\n",
    "        return RT_ms\n",
    "\n",
    "    # If there is no end time, then it means the software was fast enough to step\n",
    "    # on to the next trial instantly after the stimulus was hidden. In this case\n",
    "    # we use the last row of the given trial.\n",
    "    elif start_time_found:\n",
    "        end_time = last_time_stamp_of_trial\n",
    "        RT_ms = (end_time - start_time) / 1000.0\n",
    "        return RT_ms\n",
    "\n",
    "    # The stimulus disappeared instantly.\n",
    "    else:\n",
    "        return \"0\"\n",
    "      \n",
    "def getAOI(left_gaze_validity, right_gaze_validity, left_gaze_X, left_gaze_Y, right_gaze_X, right_gaze_Y):\n",
    "    \"\"\" Checks in which AOI of the four was the gaze \"\"\"\n",
    "    \n",
    "    if left_gaze_validity and right_gaze_validity:\n",
    "        X = (left_gaze_X + right_gaze_X) / 2.0\n",
    "        Y = (left_gaze_Y + right_gaze_Y) / 2.0\n",
    "    elif left_gaze_validity:\n",
    "        X = left_gaze_X\n",
    "        Y = left_gaze_Y\n",
    "    elif right_gaze_validity:\n",
    "        X = right_gaze_X\n",
    "        Y = right_gaze_Y\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    if X <= 0.5 and Y <= 0.5:\n",
    "        return 1\n",
    "    elif X >= 0.5 and Y <= 0.5:\n",
    "        return 2\n",
    "    elif X <= 0.5 and Y >= 0.5:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def generateOutput(raw_file_name, new_file_name, RT_data, last_AOI_data):\n",
    "    \"\"\"\n",
    "    Generates and saves the trial-level dataset with reaction times and last AOI information.\n",
    "    \n",
    "    Parameters:\n",
    "        raw_file_name (str): Path to the raw data file.\n",
    "        new_file_name (str): Path to save the generated output CSV.\n",
    "        RT_data (list): Computed reaction times.\n",
    "        last_AOI_data (list): Computed last AOI values.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    required_cols = ['computer_name', 'monitor_width_pixel', 'monitor_height_pixel', 'subject_group',\n",
    "                                          'subject_number', 'subject_sex', 'subject_age', 'asrt_type', 'PCode', 'session',\n",
    "                                          'epoch', 'block', 'trial', 'frame_rate', 'frame_time', 'frame_sd', 'stimulus_color',\n",
    "                                          'trial_type_pr', 'triplet_type_hl', 'stimulus']\n",
    "    \n",
    "    try:\n",
    "        # Read file with only necessary columns\n",
    "        input_data = pd.read_csv(raw_file_name, sep='\\t', encoding=\"latin1\", usecols=required_cols, dtype={'PCode': 'str' })\n",
    "    \n",
    "    except ValueError as e:  # Catches missing column errors\n",
    "        print(f\"Error: Missing required columns for this participant in generateOUTPUT. Skipping to next.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None  # Return None to indicate failure, allowing the loop to continue\n",
    "\n",
    "    \n",
    "    input_data = input_data.replace(',', '.', regex=True)\n",
    "    output_data = pd.DataFrame(columns=input_data.columns)\n",
    "\n",
    "    output_data = input_data[input_data['block'] != 0].drop_duplicates(subset=['epoch', 'block', 'trial'])\n",
    "\n",
    "    #output_index = 0\n",
    "    #last_trial = \"0\"\n",
    "    #for index, row in input_data.iterrows():\n",
    "    #    # we ignore 0 indexed blocks, which are calibration validation blocks.\n",
    "    #    if str(row['block']) == \"0\":\n",
    "    #        continue\n",
    "#\n",
    "    #    # insert one row of each trial data\n",
    "    #    if row['trial'] != last_trial:\n",
    "    #        last_trial = row['trial']\n",
    "    #        output_data.loc[output_index] = row\n",
    "    #        output_index += 1\n",
    "\n",
    "    # reaction time of the trial\n",
    "    \n",
    "    if len(output_data.index) == len(RT_data):\n",
    "        output_data['RT'] = RT_data\n",
    "    else:\n",
    "        RT_data = [None] * len(output_data.index)\n",
    "        output_data['RT'] = RT_data\n",
    "        warnings.warn(\"The length of your data table and RT data do not match. A list of NaN added to the data. Check the calcRTColumn function.\")\n",
    "\n",
    "    # Last AOI during RSI (useful for anticipation data calculation)\n",
    "    \n",
    "    if len(output_data.index) == len(last_AOI_data):\n",
    "        output_data['last_AOI_before_stimulus'] = last_AOI_data\n",
    "    else:\n",
    "        last_AOI_data = [None] * len(output_data.index)\n",
    "        output_data['last_AOI_before_stimulus'] = last_AOI_data\n",
    "        warnings.warn(\"The length of your data table and lastAOI data do not match. A list of NaN added to the data. Check the calcLastAOIColumn function.\")\n",
    "\n",
    "\n",
    "    output_data.to_csv(new_file_name, sep='\\t', index=False)\n",
    "    \n",
    "\n",
    "#extended trial data\n",
    "\n",
    "def computeHighLowBasedOnLearningSequence(data_table, vars_dict):\n",
    "    high_low_column = []\n",
    "    #TODO gotta switch those to not hardcoded var names\n",
    "    stimulus_column = data_table[\"stimulus\"]\n",
    "    trial_column = data_table[\"trial\"]\n",
    "    \n",
    "\n",
    "    learning_epoch = vars_dict[\"OSepoch\"]\n",
    "    epoch_data = data_table.loc[data_table[\"epoch\"] == learning_epoch]\n",
    "\n",
    "    # get the learning sequence\n",
    "    learning_sequence = findSequence(epoch_data, vars_dict[\"PCode\"], vars_dict[\"notSeq\"])\n",
    "    if learning_sequence == \"\":\n",
    "        warnings.warn(\n",
    "            \"Warning: could not find a valid learning sequence in the data for this subject. They might haven't finished the whole session\")\n",
    "        return []\n",
    "    else:\n",
    "        learning_sequence += learning_sequence[0]\n",
    "\n",
    "        # We calculate wether the current triplet is a high or low triplet based on the learning sequence.\n",
    "        for i in range(len(stimulus_column)):\n",
    "            # Can't calculate for the first two trials of the block, because there is no triplet we can use.\n",
    "            if trial_column[i] <= vars_dict[\"randoms\"]:\n",
    "                high_low_column.append('none')\n",
    "            elif (str(stimulus_column[i - 2]) + str(stimulus_column[i])) in learning_sequence:\n",
    "                high_low_column.append('high')\n",
    "            else:\n",
    "                high_low_column.append('low')\n",
    "\n",
    "        return high_low_column\n",
    "\n",
    "def computeTrillColumn(data_table):\n",
    "    trill_column = []\n",
    "    stimulus_column = data_table[\"stimulus\"]\n",
    "    trial_column = data_table[\"trial\"]\n",
    "\n",
    "    # A trial is a trill if the first and third item of the current triplet has the same\n",
    "    # stimulus, but the middle item of the triplet is different.\n",
    "    for i in range(len(stimulus_column)):\n",
    "        # Can't calculate for the first two trials of the block, because there is no triplet we can use.\n",
    "        if trial_column[i] <= 2:\n",
    "            trill_column.append('none')\n",
    "        elif (stimulus_column[i] != stimulus_column[i - 1] and\n",
    "              stimulus_column[i] == stimulus_column[i - 2]):\n",
    "            trill_column.append(True)\n",
    "        else:\n",
    "            trill_column.append(False)\n",
    "\n",
    "    return trill_column\n",
    "\n",
    "def computeRepetitionColumn(data_table):\n",
    "    repetition_column = []\n",
    "    stimulus_column = data_table[\"stimulus\"]\n",
    "    trial_column = data_table[\"trial\"]\n",
    "\n",
    "    # A trial is repetition if the previous trial has the same stimulus\n",
    "    for i in range(len(stimulus_column)):\n",
    "        # Can't calculate for the first trial of the block, because there is no previous trial.\n",
    "        if trial_column[i] <= 1:\n",
    "            repetition_column.append('none')\n",
    "        elif stimulus_column[i] == stimulus_column[i - 1]:\n",
    "            repetition_column.append(True)\n",
    "        else:\n",
    "            repetition_column.append(False)\n",
    "\n",
    "    return repetition_column\n",
    "\n",
    "def computeAnticipationColumn(data_table):\n",
    "\n",
    "    anticipation_column = []\n",
    "    stimulus_column = data_table[\"stimulus\"]\n",
    "    last_AOI_column = data_table[\"last_AOI_before_stimulus\"]\n",
    "    trial_column = data_table[\"trial\"]\n",
    "\n",
    "    # We calculate wether the last AOI during the current RSI was different from the\n",
    "    # AOI of the last trial. So the the eye was moved after the last trial.\n",
    "    for i in range(len(stimulus_column)):\n",
    "        # Can't calculate for the first trial of the block, because there is no previous trial.\n",
    "        if trial_column[i] <= 1:\n",
    "            anticipation_column.append('none')\n",
    "        # There is no valid AOI data recorded during RSI.\n",
    "        elif last_AOI_column[i] == 'none':\n",
    "            anticipation_column.append(False)\n",
    "        elif int(last_AOI_column[i]) != int(stimulus_column[i - 1]):\n",
    "            anticipation_column.append(True)\n",
    "        else:\n",
    "            anticipation_column.append(False)\n",
    "\n",
    "    return anticipation_column\n",
    "\n",
    "def findSequence(epoch_data_table, PCode_var, noPattern):\n",
    "    PCode_column = epoch_data_table[PCode_var]\n",
    "\n",
    "    PCode_list = PCode_column.unique().astype(str)\n",
    "\n",
    "    if np.isin(noPattern, PCode_list):\n",
    "        PCode_list = np.delete(PCode_list, np.where(PCode_list == noPattern))  # safer than .remove()\n",
    "\n",
    "    if PCode_list.size == 0:\n",
    "        warnings.warn(\"There's no valid PCode for this subject.\")\n",
    "        return \"\"\n",
    "    elif PCode_list.size > 1:\n",
    "        warnings.warn(\"There's more than one PCode for this subject.\")\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(PCode_list[0])\n",
    "\n",
    "def computeLearntAnticipationColumn(data_table, vars_dict, is_learning_seq):\n",
    "    learnt_anticipation_data = []\n",
    "    #TODO: switch these to non-hardcoded var names from vars_dict\n",
    "    stimulus_column = data_table[\"stimulus\"]\n",
    "    last_AOI_column = data_table[\"last_AOI_before_stimulus\"]\n",
    "    trial_column = data_table[\"trial\"]\n",
    "\n",
    "\n",
    "    if is_learning_seq:\n",
    "        epoch_for_finding_seq = vars_dict[\"OSepoch\"]\n",
    "    else:\n",
    "        if vars_dict[\"IFepoch\"] == None:\n",
    "            warnings.warn(\"This design doesn't have an interference epoch. Computation of interference anticipation is terminated\")\n",
    "            return []\n",
    "        else:\n",
    "            epoch_for_finding_seq = vars_dict[\"IFepoch\"]\n",
    "\n",
    "    epoch_data_table = data_table.loc[data_table[\"epoch\"] == epoch_for_finding_seq]\n",
    "\n",
    "    sequence = findSequence(epoch_data_table, vars_dict[\"PCode\"], vars_dict[\"notSeq\"])\n",
    "\n",
    "    if sequence == \"\":\n",
    "        warnings.warn(\n",
    "            \"Warning: could not find a valid learning sequence in the data for this subject. They might haven't finished the whole session\")\n",
    "        return []\n",
    "    else:\n",
    "        sequence += sequence[0]\n",
    "\n",
    "        for i in range(len(stimulus_column)):\n",
    "            # Can't calculate for the first two trials of the block, because there is no triplet we can use.\n",
    "            if trial_column[i] <= vars_dict[\"randoms\"]:\n",
    "                learnt_anticipation_data.append('none')\n",
    "            # There is no valid AOI data recorded during RSI.\n",
    "            elif last_AOI_column[i] == 'none':\n",
    "                learnt_anticipation_data.append(False)\n",
    "            # No anticipation eye movement. Eye is in the same AOI where the previous stimulus was.\n",
    "            elif int(last_AOI_column[i]) == int(stimulus_column[i - 1]):\n",
    "                learnt_anticipation_data.append(False)\n",
    "            # The last registered AOI during RSI follows the learning sequence.\n",
    "            elif str(stimulus_column[i - 2]) + str(last_AOI_column[i]) in sequence:\n",
    "                learnt_anticipation_data.append(True)\n",
    "            else:\n",
    "                learnt_anticipation_data.append(False)\n",
    "\n",
    "        return learnt_anticipation_data\n",
    "\n",
    "def extendTrialLevelDataForOneSubject(input_file, output_file, vars_dict):\n",
    "    data_table = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # previous trial has the stimulus at the same position -> repetition.\n",
    "    repetition_data = computeRepetitionColumn(data_table)\n",
    "    if len(repetition_data) == len(data_table.index):\n",
    "        data_table[\"repetition\"] = repetition_data\n",
    "    else:\n",
    "        repetition_data = [None] * len(data_table.index)\n",
    "        data_table[\"repetition\"] = repetition_data\n",
    "        warnings.warn(\"The length of your data table and repetition data do not match. A list of NaN addet to the repetition col. Check the computeRepetitionColumn function.\")\n",
    "\n",
    "\n",
    "\n",
    "    # trill: first item and third item of trial triplet is the same: e.g. 1x1, 2x2, etc.\n",
    "    trill_data = computeTrillColumn(data_table)\n",
    "    if len(trill_data) == len(data_table.index):\n",
    "        data_table[\"trill\"] = trill_data\n",
    "    else:\n",
    "        trill_data = [None] * len(data_table.index)\n",
    "        data_table[\"trill\"] = trill_data\n",
    "        warnings.warn(\"The length of your data table and trill data do not match. A list of NaN addet to the trill col. Check the computeTrillColumn function.\")\n",
    "\n",
    "    # calculate frequency based on learning sequence\n",
    "    high_low_data = computeHighLowBasedOnLearningSequence(data_table, vars_dict)\n",
    "    if len(high_low_data) == len(data_table.index):\n",
    "        data_table[\"high_low_learning\"] = high_low_data\n",
    "    else:\n",
    "        high_low_data = [None] * len(data_table.index)\n",
    "        data_table[\"high_low_learning\"] = high_low_data\n",
    "        warnings.warn(\"The length of your data table and HL learning data do not match. A list of NaN addet to the high_low_learning col. Check the computeHighLowBasedOnLearningSequence function.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # calculate whether anticipatory eye-movement has happened\n",
    "    anticipation_data = computeAnticipationColumn(data_table)\n",
    "    if len(anticipation_data) == len(data_table.index):\n",
    "        data_table[\"has_anticipation\"] = anticipation_data\n",
    "    else:\n",
    "        anticipation_data = [None] * len(data_table.index)\n",
    "        data_table[\"has_anticipation\"] = anticipation_data\n",
    "        warnings.warn(\"The length of your data table and anticipation data do not match. A list of NaN addet to the has_anticipation col. Check the computeAnticipationColumn function.\")\n",
    "\n",
    "    # calculate whether learning dependent anticipatory eye-movement has happened\n",
    "\n",
    "    learnt_anticipation_data_OS = computeLearntAnticipationColumn(data_table, vars_dict, True)\n",
    "    if len(learnt_anticipation_data_OS) == len(data_table.index):\n",
    "        data_table[\"has_learnt_anticipation_OS\"] = learnt_anticipation_data_OS\n",
    "    else:\n",
    "        learnt_anticipation_data_OS = [None] * len(data_table.index)\n",
    "        data_table[\"has_learnt_anticipation_OS\"] = learnt_anticipation_data_OS\n",
    "        warnings.warn(\"The length of your data table and has_learnt_anticipation_original_seq data do not match. A list of NaN addet to the has_learnt_anticipation_original_seq col. Check the computeLearntAnticipationColumn function.\")\n",
    "\n",
    "    if vars_dict[\"IFepoch\"] != None:\n",
    "        learnt_anticipation_interference = computeLearntAnticipationColumn(data_table, vars_dict, False)\n",
    "        if len(learnt_anticipation_interference) == len(data_table.index):\n",
    "            data_table[\"has_learnt_anticipation_IF\"] = learnt_anticipation_interference\n",
    "        else:\n",
    "            learnt_anticipation_interference = [None] * len(data_table.index)\n",
    "            data_table[\"has_learnt_anticipation_IF\"] = learnt_anticipation_interference\n",
    "            warnings.warn(\"The length of your data table and has_learnt_anticipation_interference data do not match. A list of NaN addet to the has_learnt_anticipation_interference col. Check the computeLearntAnticipationColumn function.\")\n",
    "\n",
    "\n",
    "    data_table.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "def extendTrialLevelData(input_dir, output_dir, vars_dict):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for subject_file in files:\n",
    "            subject = subject_file.split('_')[1]\n",
    "\n",
    "            print(\"Extend trial level data with additional fields for subject: \" + subject)\n",
    "\n",
    "            input_file = os.path.join(input_dir, subject_file)\n",
    "            output_file = os.path.join(output_dir, 'subject_' + subject + '__trial_extended_log.csv')\n",
    "            extendTrialLevelDataForOneSubject(input_file, output_file, vars_dict)\n",
    "\n",
    "        break\n",
    "        \n",
    "        \n",
    "#cleanup to create HL LMM-compatible data + ofc our beloved simple pivot\n",
    "\n",
    "def clean_and_organize(list_cols, variables, sub_to_drop, ext_trialdata_path):\n",
    "    ''' \n",
    "    function to arrange everything about the data: drop unneeded cols, convert RT to numeric, drop excl subjects, drop random trials from the beginning of blocks\n",
    "    INPUT: raw ext trial data, list of cols needed, variables dict as defined above, list of subjects to exclcude\n",
    "    OUTPUT: raw data with cleaned columns, RT as num, dropped subjects\n",
    "    '''\n",
    "    \n",
    "    ## first, read all file names in ext_trialdat_path\n",
    "    all_files = glob.glob(os.path.join(ext_trialdata_path, '*.csv'))\n",
    "    \n",
    "    # read & concat them\n",
    "    raw_dat = pd.concat((pd.read_csv(f, sep = '\\t') for f in all_files), ignore_index=True)\n",
    "    \n",
    "    blockprepN = variables[\"randoms\"]\n",
    "    trial = variables[\"trial\"]\n",
    "    unwanted_col = False\n",
    "    ## step1: drop not needed cols\n",
    "    #check list_cols\n",
    "    for col in list_cols:\n",
    "        if col not in raw_dat.columns:\n",
    "            print(f'{col} not in data.')\n",
    "            unwanted_col = True\n",
    "    if not unwanted_col:\n",
    "        raw_dat = raw_dat[list_cols]\n",
    "    else:\n",
    "        print(\"couldn't do col cleaning. Check the list_cols and try again.\")\n",
    "\n",
    "\n",
    "    ##step2\n",
    "    #if RT data is not numeric, change it\n",
    "    if raw_dat[variables[\"dep\"]].dtypes != 'float64':\n",
    "        raw_dat[variables[\"dep\"]] = pd.to_numeric(raw_dat[variables[\"dep\"]].str.replace(',', '.'))\n",
    "    \n",
    "    #step3: drop subjects\n",
    "    if len(sub_to_drop) < 0:\n",
    "        raw_dat.drop(raw_dat.loc[raw_dat[variables[\"ID\"]].isin(sub_to_drop)].index, inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    return raw_dat\n",
    "\n",
    "def drop_blockstarting_randoms(trial_data, vars_dict):\n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    trial = vars_dict[\"trial\"]\n",
    "    #drop random trials in the beginning of each epoch\n",
    "    cleaned_data = trial_data.loc[trial_data[trial] > 5].copy()\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def get_trialdat_ready_for_HL_LMM(raw_dat_clean, vars_dict, drop0RT = True, is_interference = True):\n",
    "    \"\"\"This function 1) deletes the first blockprepN trials that are not sequential, \n",
    "    create integer of the has learnt anticipation column (if is_interference == True, for both OS and IF anticipation cols)\n",
    "    if drop0RT == True it drops null reactiontimes\n",
    "    drop trills and reps\"\"\"\n",
    "    raw_dat_clean = raw_dat_clean.copy()\n",
    "    RT = vars_dict[\"dep\"]\n",
    "\n",
    "    #1) drop first few trials\n",
    "    blockprepN = vars_dict[\"randoms\"]\n",
    "    raw_dat_clean = raw_dat_clean.loc[raw_dat_clean[\"trial\"]>blockprepN]\n",
    "\n",
    "    #2) whether has anticipation and has learnt anticipation cols are string or bool, these lines will code it as 0 and 1\n",
    "    raw_dat_clean[\"has_anticipation_int\"] = raw_dat_clean[\"has_anticipation\"].map({\"True\": 1, \"False\": 0, True: 1, False: 0, \"TRUE\": 1, \"FALSE\": 0})\n",
    "    raw_dat_clean[\"has_learnt_anticipation_OS_int\"] = raw_dat_clean[\"has_learnt_anticipation_OS\"].map({\"True\": 1, \"False\": 0, True: 1, False: 0, \"TRUE\": 1, \"FALSE\": 0})\n",
    "    if vars_dict[\"IFepoch\"] != None:\n",
    "        raw_dat_clean[\"has_learnt_anticipation_IF_int\"] = raw_dat_clean[\"has_learnt_anticipation_IF\"].map({\"True\": 1, \"False\": 0, True: 1, False: 0, \"TRUE\": 1, \"FALSE\": 0})\n",
    "\n",
    "    #3) drop null reaction times if requested\n",
    "    if drop0RT:\n",
    "        raw_dat_clean = drop_blockstarting_randoms(raw_dat_clean, vars_dict)\n",
    "\n",
    "    #4) drop trills and reps\n",
    "    true_list =  [True, \"TRUE\", \"True\"]\n",
    "    raw_dat_clean = raw_dat_clean.loc[~raw_dat_clean[\"trill\"].isin(true_list)]\n",
    "    raw_dat_clean = raw_dat_clean.loc[~raw_dat_clean[\"repetition\"].isin(true_list)]\n",
    "\n",
    "    return raw_dat_clean\n",
    "\n",
    "def simple_pivot(data_in, outcome_var, indeces, cols, aggfunct):\n",
    "    data_out = pd.pivot_table(data_in, values = outcome_var, index = indeces, columns = cols, aggfunc = aggfunct)\n",
    "    data_out.columns = [' '.join(str(col)).strip() for col in data_out.columns.values]\n",
    "    data_out.reset_index(inplace=True)\n",
    "    data_out.columns = data_out.columns.str.replace(\"[()'', ]\", \"\", regex = True)\n",
    "    data_out.replace('', np.nan, inplace=True)\n",
    "    data_out.dropna(axis = 0, inplace=True)\n",
    "    if \"group_string\" in data_in.columns:\n",
    "        groupfile = data_in[['Subject', \"group\", \"group_string\"]].drop_duplicates()\n",
    "        data_out = pd.merge(data_out, groupfile, on='Subject', how='outer')\n",
    "    return data_out\n",
    "\n",
    "def calcHighLowDiff(data_wide, vars_dict, is_RT = False):\n",
    "    high = vars_dict[\"H\"]\n",
    "    low = vars_dict[\"L\"]\n",
    "    data_for_HLdiff = data_wide[[col for col in data_wide.columns if high in col or low in col]]\n",
    "    #if data_for_HLdiff.columns[0] != '2high' or data_for_HLdiff.columns[1] != '2low':\n",
    "    #    raise Exception(\"Column name structure is not suitable. Please rename to [2high, 2low, 3high, 3low....]\")\n",
    "    for col in data_for_HLdiff:\n",
    "        if \"high\" in col:\n",
    "            previous_col = col\n",
    "        elif \"low\" in col:\n",
    "            if is_RT:\n",
    "                data_wide[\"TRI_\" + col[0]] = data_for_HLdiff[col] - data_for_HLdiff[previous_col]\n",
    "            else:\n",
    "                data_wide[\"TRI_\" + col[0]] = data_for_HLdiff[previous_col] - data_for_HLdiff[col]\n",
    "            previous_col = col\n",
    "    return data_wide\n",
    "\n",
    "\n",
    "#processing the IF epoch only:\n",
    "def calculate_HLLH_RT(dat_IF_epochs, vars_dict):\n",
    "    \"\"\"\n",
    "    Compute HLLH_RT based on 'high_low_learning' and 'triplet_type_hl' values.\n",
    "\n",
    "    Parameters:\n",
    "    - dat_IF_epochs: DataFrame with IF epochs\n",
    "    - vars_dict: Dictionary containing 'H' and 'L' keys\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with 'HLLH_RT' column\n",
    "    \"\"\"\n",
    "    HLLH_list = []\n",
    "    \n",
    "    for _, row in dat_IF_epochs.iterrows():\n",
    "        if row[\"high_low_learning\"] == vars_dict[\"H\"] and row[\"triplet_type_hl\"] == vars_dict[\"H\"]:\n",
    "            HLLH_list.append(\"HH\")\n",
    "        elif row[\"high_low_learning\"] == vars_dict[\"H\"] and row[\"triplet_type_hl\"] == vars_dict[\"L\"]:\n",
    "            HLLH_list.append(\"HL\")\n",
    "        elif row[\"high_low_learning\"] == vars_dict[\"L\"] and row[\"triplet_type_hl\"] == vars_dict[\"H\"]:\n",
    "            HLLH_list.append(\"LH\")\n",
    "        elif row[\"high_low_learning\"] == vars_dict[\"L\"] and row[\"triplet_type_hl\"] == vars_dict[\"L\"]:\n",
    "            HLLH_list.append(\"LL\")\n",
    "        else:\n",
    "            HLLH_list.append(\"none\")\n",
    "\n",
    "    if len(HLLH_list) != dat_IF_epochs.shape[0]:\n",
    "        raise IndexError(\"Mismatch between DataFrame length and computed HLLH list.\")\n",
    "\n",
    "    dat_IF_epochs.loc[:, \"HLLH_RT\"] = HLLH_list\n",
    "    return dat_IF_epochs\n",
    "\n",
    "def calculate_HLLH_LDAEM(dat_IF_epochs, vars_dict):\n",
    "    \"\"\"\n",
    "    Compute HLLH_LDAEM based on 'has_learnt_anticipation_OS' and 'has_learnt_anticipation_IF' values.\n",
    "\n",
    "    Parameters:\n",
    "    - dat_IF_epochs: DataFrame with IF epochs\n",
    "    - vars_dict: Dictionary containing 'H' and 'L' keys\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with 'HLLH_LDAEM' column\n",
    "    \"\"\"\n",
    "    LDNLD_list = []\n",
    "\n",
    "    for _, row in dat_IF_epochs.iterrows():\n",
    "        if row[\"has_learnt_anticipation_OS\"] == \"True\" and row[\"has_learnt_anticipation_IF\"] == \"True\":\n",
    "            LDNLD_list.append(\"LD-LD\")\n",
    "        elif row[\"has_learnt_anticipation_OS\"] == \"True\" and row[\"has_learnt_anticipation_IF\"] == \"False\":\n",
    "            LDNLD_list.append(\"LD-NLD\")\n",
    "        elif row[\"has_learnt_anticipation_OS\"] == \"False\" and row[\"has_learnt_anticipation_IF\"] == \"True\":\n",
    "            LDNLD_list.append(\"NLD-LD\")\n",
    "        elif row[\"has_learnt_anticipation_OS\"] == \"False\" and row[\"has_learnt_anticipation_IF\"] == \"False\":\n",
    "            LDNLD_list.append(\"NLD-NLD\")\n",
    "        else:\n",
    "            LDNLD_list.append(\"none\")\n",
    "\n",
    "    if len(LDNLD_list) != dat_IF_epochs.shape[0]:\n",
    "        raise IndexError(\"Mismatch between DataFrame length and computed LDNLD list.\")\n",
    "\n",
    "    dat_IF_epochs.loc[:, \"HLLH_LDAEM\"] = LDNLD_list\n",
    "    return dat_IF_epochs\n",
    "\n",
    "def process_IF_epochs(raw_dat_clean, vars_dict):\n",
    "    \"\"\"\n",
    "    Filters IF epochs and computes HLLH_RT and HLLH_LDAEM.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dat_clean: Raw DataFrame\n",
    "    - vars_dict: Dictionary containing epoch information and labels ('H', 'L')\n",
    "\n",
    "    Returns:\n",
    "    - Processed DataFrame with new columns 'HLLH_RT' and 'HLLH_LDAEM'\n",
    "    \"\"\"\n",
    "    seqB_list = vars_dict[\"seqB_list\"]\n",
    "    if len(seqB_list) == 0:\n",
    "        print(\"You didn't even have interference epochs here. Calculating HLLH scores failed miserably.\")\n",
    "        return None\n",
    "    epoch = vars_dict[\"epoch\"]\n",
    "    dat_IF_epochs = raw_dat_clean.loc[raw_dat_clean[epoch].isin(seqB_list)].copy()\n",
    "\n",
    "    dat_IF_epochs = calculate_HLLH_RT(dat_IF_epochs, vars_dict)\n",
    "    dat_IF_epochs = calculate_HLLH_LDAEM(dat_IF_epochs, vars_dict)\n",
    "\n",
    "    return dat_IF_epochs\n",
    "\n",
    "#generating response type files\n",
    "\n",
    "def filter_random_epochs(raw_data, variables_dict):\n",
    "    out_dat = raw_data.loc[raw_data[variables_dict['PCode']] != variables_dict['notSeq']].copy()\n",
    "    return out_dat\n",
    "\n",
    "def is_response_LD(PCode, current_triplet, response):\n",
    "    \"\"\"\n",
    "    checks if the given response is learning-dependent (corresponding to a H triplet)\n",
    "    INPUT: sequence code e.g. 1234, the triplet of which the current stim is the last element of, current last AOI\n",
    "    OUTPUT: bool whether it's learning-dependent or not OR None if something is wrong\n",
    "    \"\"\"\n",
    "    #returns True if response is LD, False if NLD\n",
    "    PCode_loop = str(PCode) + str(PCode)\n",
    "    \n",
    "    if len(str(current_triplet)) <3:\n",
    "        raise IndexError('Uh-oh, it seems your triplet isnt long enough if you know what I mean.')\n",
    "        return None\n",
    "    \n",
    "    resp_PCode = str(current_triplet)[0] + str(response)\n",
    "    \n",
    "    if resp_PCode in PCode_loop:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_response_correct(response, stimulus):\n",
    "    \"\"\"\n",
    "    calculates if the given response corresponds to the stim appearing\n",
    "    \"\"\"\n",
    "    \n",
    "    if str(response) == str(-1):\n",
    "        print(\"no interpretable lastAOI. Skipping this one.\")\n",
    "        return None\n",
    "    \n",
    "    if str(response) == str(stimulus):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def update_triplet(current_triplet, stim):\n",
    "    if len(str(current_triplet))<3:\n",
    "        return str(current_triplet) + str(stim)\n",
    "    else:\n",
    "        return str(current_triplet)[1:3] + str(stim)\n",
    "    \n",
    "def response_data_calculator(raw_data_in, variables_dict, is_interference=False):\n",
    "    \"\"\"\n",
    "    This function calculates the four response types for each trial.\n",
    "    INPUT: raw ext trial data with filtered cols and subjects (as in clean_and_organize); \n",
    "        variables dictionary as defined above. \n",
    "        If you check an interference epoch, set is_interference = True\n",
    "    OUTPUT: raw ext trial data with response_type/original_response_type (if is_interference == True), and current_triplet cols\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # drop nonseq epochs\n",
    "    raw_data = filter_random_epochs(raw_data_in, variables_dict)\n",
    "       \n",
    "    # init vars needed\n",
    "    if not is_interference:\n",
    "        PCodevar = variables_dict['PCode']\n",
    "    else:\n",
    "        PCodevar = variables_dict['original_PCode']\n",
    "   \n",
    "    response_type = []\n",
    "    triplet = []\n",
    "    current_triplet = ''  # Initialize current_triplet outside the loop\n",
    "    trial = variables_dict['trial']\n",
    "    stim = variables_dict['stim']\n",
    "    lastAOI = variables_dict['response']\n",
    "    \n",
    "    for index, row in raw_data.iterrows():\n",
    "        \n",
    "        # Checking if we are at a new block, if so, reset current_triplet\n",
    "        if int(row[trial]) == 1:\n",
    "            current_triplet = ''\n",
    "        else:\n",
    "            pass\n",
    "       \n",
    "        # Update triplet\n",
    "        current_triplet = update_triplet(current_triplet, row[stim])\n",
    "        triplet.append(current_triplet)\n",
    "       \n",
    "        if len(current_triplet) < 3:\n",
    "            response_type.append('none')\n",
    "            \n",
    "        else:\n",
    "            if is_response_correct(row[lastAOI], row[stim]):\n",
    "                if is_response_LD(row[PCodevar], current_triplet, row[lastAOI]):\n",
    "                    response_type.append('LD_CORRECT')\n",
    "                elif not is_response_LD(row[PCodevar], current_triplet, row[lastAOI]):\n",
    "                    response_type.append('NLD_CORRECT')\n",
    "                else:\n",
    "                    print(f'Failed to calculate resp type at row {index}, epoch {row[variables_dict[\"epoch\"]]} trial {trial}')\n",
    "                    response_type.append(\"None\")\n",
    "            \n",
    "            elif not is_response_correct(row[lastAOI], row[stim]):\n",
    "                if is_response_LD(row[PCodevar], current_triplet, row[lastAOI]):\n",
    "                    response_type.append('LD_ERROR')\n",
    "                elif not is_response_LD(row[PCodevar], current_triplet, row[lastAOI]):\n",
    "                    response_type.append('NLD_ERROR')\n",
    "                else:\n",
    "                    print(f'Failed to calculate resp type at row {index}, epoch {row[variables_dict[\"epoch\"]]} trial {trial}')\n",
    "                    response_type.append(\"None\")\n",
    "            \n",
    "            else: \n",
    "                print(f'Failed to calculate resp type at row {index}, epoch {row[variables_dict[\"epoch\"]]} trial {trial}')\n",
    "                response_type.append(\"None\")\n",
    "                \n",
    "\n",
    "    if not is_interference:\n",
    "        raw_data['response_type'] = response_type\n",
    "        raw_data['current_triplet'] = triplet\n",
    "    else:\n",
    "        raw_data['original_response_type'] = response_type\n",
    "       \n",
    "    return raw_data\n",
    "\n",
    "\n",
    "#likelihoods\n",
    "\n",
    "def count_values_epochwise(data, variables_dict, counted_var):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ### create variables of col names\n",
    "    ID = variables_dict[\"ID\"]\n",
    "    epoch = variables_dict[\"epoch\"]\n",
    "    ### create a column full of ones only\n",
    "    data[\"count_support\"] = 1\n",
    "    ### calculating how many are there of the resp types\n",
    "    trial_total_counts = simple_pivot(data, \"count_support\", ID, epoch, \"sum\")\n",
    "    counts = simple_pivot(data, \"count_support\", ID, [epoch, counted_var], \"sum\")\n",
    "    \n",
    "    return counts, trial_total_counts\n",
    "\n",
    "def get_percentage_data(counts, trial_total_counts, variables_dict):\n",
    "    \"\"\"\n",
    "    This function calculates what percentage of the trials in a given epoch were a given response type.\n",
    "    INPUT: wide response count data as produced by count_epochwise, wide trial count data (epoch nr as column), variables dictionary to define col names\n",
    "    OUTPUT: wide response percentage data\n",
    "    \"\"\"\n",
    "    ID = variables_dict['ID']\n",
    "    dat_percentage = pd.DataFrame(columns = counts.columns)\n",
    "    dat_percentage[ID] = counts[ID]\n",
    "    for col in counts.columns:\n",
    "        for col2 in trial_total_counts.columns:\n",
    "            if col2 == ID:\n",
    "                pass\n",
    "            elif col2 in col:\n",
    "                dat_percentage[col] = counts[col]/trial_total_counts[col2]\n",
    "    return dat_percentage\n",
    "\n",
    "def divide_cols(dat_percentage, variables, probability_dict):\n",
    "    \"\"\"\n",
    "    divides colums with corresponding probabilities\n",
    "    INPUT: wide (response) percentage data, variable dictionary, probabilities in dictionary form (default: the resp type probabilities)\n",
    "    OUTPUT: the new shiny likelihood ratio wide data\n",
    "    \"\"\"\n",
    "    ID = variables['ID']\n",
    "    random = variables['rando']\n",
    "    \n",
    "    dat_LLR = pd.DataFrame(columns=dat_percentage.columns)\n",
    "    for colname, col in dat_percentage.items():\n",
    "        if colname == ID:\n",
    "            dat_LLR[colname] = dat_percentage[colname]\n",
    "        elif random in colname:\n",
    "            pass\n",
    "        else:\n",
    "            for key in probability_dict:\n",
    "                if key in colname and len(colname) == len(key)+1:\n",
    "                    #(this latter bit cuz LD_ERROR is in NLD_ERROR too :/ )\n",
    "                    dat_LLR[colname] = dat_percentage[colname]/probability_dict[key]\n",
    "                else:\n",
    "                    pass\n",
    "    return dat_LLR\n",
    "\n",
    "def likelihood_generator(trialdata, variables_dict, count_variable, probability_dict = {\"LD_CORRECT\": 0.15625, \"LD_ERROR\": 0.09375,\"NLD_ERROR\": 0.65625,\"NLD_CORRECT\": 0.09375}):\n",
    "    \"\"\"\n",
    "    This function creates the likelihood data, as in, P(response type|baseline probability of given resp type).\n",
    "    INPUT: trial-based response type data, variables dictionary, and if you calculate for anything but response types then the probabilities of options in a dictionary\n",
    "    OUTPUT: the absolutely beautiful likelihood ratio data (wide)\n",
    "    \"\"\"\n",
    "    \n",
    "    dat_resp_type, dat_epochtrial = count_values_epochwise(trialdata, variables_dict, count_variable)\n",
    "    \n",
    "    percdata = get_percentage_data(dat_resp_type, dat_epochtrial, variables_dict)\n",
    "    \n",
    "    LLR_data = divide_cols(percdata, variables_dict, probability_dict)\n",
    "    \n",
    "    return LLR_data\n",
    "\n",
    "#updates\n",
    "\n",
    "def was_there_an_update(tri_dict, row, lastAOI):\n",
    "    current = str(row[\"current_triplet\"])\n",
    "    if current not in tri_dict:\n",
    "        raise ValueError(f'Man, we dont have any {current} in the triplet dictionary, check the is_prior_updated_function or dunno')\n",
    "        return pd.NA\n",
    "    elif tri_dict[current][0] == row[lastAOI]:\n",
    "        #there was no update\n",
    "        return False\n",
    "    elif row[lastAOI] == -1:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        return True  \n",
    "    \n",
    "def update_type(row, update_type, prev_resp):\n",
    "    if update_type not in [True, False]:\n",
    "        return \"\"\n",
    "    elif update_type == False:\n",
    "        if \"NLD\" in row[\"response_type\"]:\n",
    "            return \"NLD_SAME\"\n",
    "        else:\n",
    "            return \"LD_SAME\"\n",
    "    elif update_type == True:\n",
    "        if row[\"response_type\"] in [\"LD_CORRECT\", \"LD_ERROR\"]:\n",
    "            return \"MOD_CORR\"\n",
    "            \n",
    "        #cuz it cannot be LD AND update unless the previous response was NLD\n",
    "        else:\n",
    "            if prev_resp in [\"LD_CORRECT\", \"LD_ERROR\"]:\n",
    "                return \"MOD_DISR\"\n",
    "            else:\n",
    "                return \"EXPL\"\n",
    "    else:\n",
    "        print(\"TypeWarning: most likely your update variable is not a bool. You may wanna check the calculate_update vars function.\")      \n",
    "        \n",
    "def calculate_update_vars(dat_with_responses, variables_dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT: unfiltered trial-by-trial response data\n",
    "    \"\"\"\n",
    "    \n",
    "    #init target vars\n",
    "    update = []\n",
    "    trials_since_triplet = []\n",
    "    LD_update = []\n",
    "    prev_response_type = []\n",
    "    current_subject = ''\n",
    "    triplets = dict()\n",
    "    ID = variables_dict[\"ID\"]\n",
    "    lastAOI = variables_dict[\"response\"]\n",
    "    \n",
    "    for index, row in dat_with_responses.iterrows():\n",
    "        \n",
    "        #vars to make code more readable:\n",
    "        if str(current_subject) != str(row[ID]):\n",
    "            triplets = dict()\n",
    "            print(f'current_subject {current_subject} updated to {row[ID]}')\n",
    "            current_subject = str(row[ID])\n",
    "        current_triplet = str(row[\"current_triplet\"])\n",
    "        response = row[lastAOI]\n",
    "        \n",
    "        if len(current_triplet)<3:\n",
    "            update.append(pd.NA)\n",
    "            trials_since_triplet.append(np.nan)\n",
    "            LD_update.append('none')\n",
    "            prev_response_type.append('none')\n",
    "        \n",
    "        elif current_triplet not in triplets:\n",
    "            triplets[current_triplet] = [response, row['response_type'], index]\n",
    "            update.append(pd.NA)\n",
    "            trials_since_triplet.append(np.nan)\n",
    "            LD_update.append('first')\n",
    "            prev_response_type.append('first')\n",
    "\n",
    "        else:\n",
    "            update_now = was_there_an_update(triplets, row, lastAOI)\n",
    "            update.append(update_now)\n",
    "            trials_since_triplet.append(index - triplets[current_triplet][2])\n",
    "            LD_update.append(update_type(row, update_now, triplets[current_triplet][1]))\n",
    "            prev_response_type.append(triplets[current_triplet][1])\n",
    "            #print(f'Im adding {triplets[current_triplet]} to the dict')\n",
    "            triplets[current_triplet] = [response, row['response_type'], index]\n",
    "            \n",
    "    #print(triplets)\n",
    "    dat_with_responses['update'] = update\n",
    "    dat_with_responses['trials_since_triplet'] = trials_since_triplet\n",
    "    dat_with_responses['update_type'] = LD_update\n",
    "    dat_with_responses['prev_resp'] = prev_response_type\n",
    "    dat_with_updates = dat_with_responses.copy()\n",
    "    \n",
    "    return dat_with_updates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad6145",
   "metadata": {},
   "source": [
    "todo: readind settings file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae760c9",
   "metadata": {},
   "source": [
    "## create folder tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144f643-e538-4878-818a-617d57f0923c",
   "metadata": {},
   "source": [
    "create required folders if they don't exist already, and save their paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ed92f2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'indat' already exists.\n",
      "Folder 'trialdata' already exists.\n",
      "Folder 'ext_trialdata' already exists.\n",
      "Folder 'DATAQUAL_missing_data_ratio' already exists.\n",
      "Folder 'DATAQUAL_distance' already exists.\n",
      "Folder 'DATAQUAL_E2E' already exists.\n",
      "Folder 'DATAQUAL_S2S' already exists.\n",
      "Folder 'anticipatory_eye_movements' already exists.\n",
      "Folder 'statistical_learning' already exists.\n",
      "Folder 'interference' already exists.\n",
      "Folder 'output' already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of folder names to be created\n",
    "folders = [\n",
    "    \"indat\", \n",
    "    \"trialdata\", \n",
    "    \"ext_trialdata\", \n",
    "    \"DATAQUAL_missing_data_ratio\", \n",
    "    \"DATAQUAL_distance\", \n",
    "    \"DATAQUAL_E2E\", \n",
    "    \"DATAQUAL_S2S\", \n",
    "    \"anticipatory_eye_movements\", \n",
    "    \"statistical_learning\", \n",
    "    \"interference\",\n",
    "    \"output\"\n",
    "]\n",
    "# Create a dictionary to store the paths\n",
    "folder_paths = {}\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Loop through the folders, create them, and store their paths\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(current_directory, folder)\n",
    "    folder_paths[f\"{folder}_path\"] = folder_path\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder}' created at: {folder_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Folder '{folder}' already exists.\")\n",
    "\n",
    "# Unpack the dictionary to assign each path as a variable\n",
    "globals().update(folder_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e0eea",
   "metadata": {},
   "source": [
    "## Put the input files in the \"indat\" folder we just created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57401490-dd05-4de8-b122-8e3a6574caeb",
   "metadata": {},
   "source": [
    "(subject-by-subject, created by PsychoPy, name structure as in \"subject_##__log\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878e57",
   "metadata": {},
   "source": [
    "## Your design \n",
    "Tell me about your design. <br>\n",
    "In the future, this should be read from the settings file but thing is I'm a lazy but time-efficient bastard so bear with me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dd1691b0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## we will use this vars dictionary for the data preparation phase.\n",
    "\n",
    "vars_dict = {\n",
    "    #add the nr of epochs in the files\n",
    "    \"epochN\": 8,\n",
    "    #how many blocks are there in one epoch?\n",
    "    \"blocks\": 5,\n",
    "    #number of practice random trials in a block\n",
    "    \"randoms\": 5,\n",
    "    #sequential trials in a block\n",
    "    \"trialN\": 80,\n",
    "    #if u use the Tobii-optimized ASRT, it's always 12:\n",
    "    \"fixation_threshold\": 12,\n",
    "    #the location of the appearing stim:\n",
    "    'stim': 'stimulus',\n",
    "    #H L triplets:\n",
    "    'triplet_type': 'triplet_type_hl',\n",
    "    #participant's response (e.g., last AOI in eye-tracking data, or button the participant pressed in the motor version):\n",
    "    'response': 'last_AOI_before_stimulus',\n",
    "    #how are random stimuli in the beginning of blocks marked?\n",
    "    'rando': 'none',\n",
    "    #the string for high triplets\n",
    "    'H': 'high',\n",
    "    #and low?\n",
    "    'L': 'low',\n",
    "    #which var is a string of the sequence (e.g., '1234')?\n",
    "    'PCode': 'PCode',\n",
    "    #IF you have interference - if not change to None:\n",
    "    'original_PCode': 'original_PCode',\n",
    "    #time unit (epoch or block)\n",
    "    'epoch': 'epoch',\n",
    "    #string marking nonpattern blocks:\n",
    "    'notSeq': 'noPattern',\n",
    "    #trial nr\n",
    "    'trial': 'trial',\n",
    "    #response type (LDC LDE NLDC NLDE) variable\n",
    "    'response_type': 'response_type',\n",
    "    #dependent var, typically RT\n",
    "    'dep': 'RT',\n",
    "    #triplet type \n",
    "    'TT': 'triplet_type_hl',\n",
    "    #subject ID\n",
    "    'ID': 'subject_number',\n",
    "    #first sequential epoch nr\n",
    "    'OSepoch': 6,\n",
    "    #IFepoch: which one is your first interference epoch? If there's no interference epoch, just write None\n",
    "    'IFepoch': 1,\n",
    "    #list of epochs with seqA\n",
    "    \"seqA_list\": [6,8],\n",
    "    #list of epochs with seqB (if none, add empty list)\n",
    "    \"seqB_list\": [1,2,3,4,5,7],\n",
    "    #HL based on OS\n",
    "    \"HL_OS_var\": \"high_low_learning\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# List of subjects with bad data\n",
    "subs_with_bad_data = {47}\n",
    "\n",
    "\n",
    "## copy here any valid file name in the indat folder. We will use it to test the settings\n",
    "testfile = \"subject_87__log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b633442",
   "metadata": {},
   "source": [
    "## Data validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bd7b0",
   "metadata": {},
   "source": [
    "Let's check if your data looks ok and throw out some bad data so you don't get fcked later on. <br>\n",
    "\n",
    "This section requires you to fill the subs_with_bad_data list with your subjects-to-drop !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecda13-b756-456d-920d-98971ad7fb69",
   "metadata": {},
   "source": [
    "### data type validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6e19f77",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All file formats are valid in the 'indat' folder.\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "junk_folder = os.path.join(os.getcwd(), \"junk\")\n",
    "os.makedirs(junk_folder, exist_ok=True)  # Ensure junk folder exists\n",
    " \n",
    "file_validator(indat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29eb3e",
   "metadata": {},
   "source": [
    "### is vars_dict correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce141c-6c38-4309-9dd5-7b0ae34ae132",
   "metadata": {},
   "source": [
    "checking if you say the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bea9063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables in 'variables_dict' are present in the DataFrame columns.\n",
      "Using the file C:\\Users\\porsi\\BML-MEMO LAB Dropbox\\bml memo members\\Orsi_Pesthy\\everuseful\\ET_py\\NEW_IMPROVED_ET_analyser\\indat/subject_87__log.txt\n",
      "Your nr of epochs match with what you specified in the vars_dict.\n",
      "The specified nr of random and sequential trials match the data structure.\n",
      "Aaand yepp, the interference and original sequence epochs indeed differ. Good. You are all set.\n"
     ]
    }
   ],
   "source": [
    "check_variables_dict(indat_path + \"/\" + testfile, vars_dict)\n",
    "data_structure_validator(vars_dict, indat_path + \"/\" + testfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde8ef6-c603-4ddf-a947-4767ede898c9",
   "metadata": {},
   "source": [
    "### drop wrong datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d98be076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good, all subjects seem to have sufficient amount of epochs\n",
      "bad sub list is: {47}\n"
     ]
    }
   ],
   "source": [
    "subject_dropper(indat_path, subs_with_bad_data, vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ecd2a",
   "metadata": {},
   "source": [
    "### stimulus col validator\n",
    " If it warns you about the getAOI function, you have to fix the coordinates belonging to each stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79c1846c-59a1-4d68-931d-25bfbca74f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: subject_100__log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\porsi\\AppData\\Local\\Temp\\ipykernel_18552\\1681700322.py:186: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: str(x).replace(\",\", \".\") if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stimuli met expectations, your getAOI function functions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: (0.36, 0.74), 1: (0.36, 0.25), 2: (0.63, 0.25), 4: (0.64, 0.74)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gaze_coordinate_coding(indat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9def30e",
   "metadata": {},
   "source": [
    "# (now, you can just mindlessly run all the cells, the following codes don't require your attention, at least they shouldn't)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b87c3d",
   "metadata": {},
   "source": [
    "# data quality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95e35f",
   "metadata": {},
   "source": [
    "## missing data ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8917f4",
   "metadata": {},
   "source": [
    "doublechecked, it works. \n",
    "\n",
    "15012025 update: triplechecked, F, C, O yeee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3c6c95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute missing data ratio for subject: 100\n",
      "Compute missing data ratio for subject: 106\n",
      "Compute missing data ratio for subject: 75\n",
      "Compute missing data ratio for subject: 87\n"
     ]
    }
   ],
   "source": [
    "computeMissingDataRatio(indat_path, DATAQUAL_missing_data_ratio_path+\"/missing_data.csv\", vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1894e",
   "metadata": {},
   "source": [
    "## distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "085bc55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute eye-screen distance data for subject: 100\n",
      "Compute eye-screen distance data for subject: 106\n",
      "Compute eye-screen distance data for subject: 75\n",
      "Compute eye-screen distance data for subject: 87\n"
     ]
    }
   ],
   "source": [
    "computeDistance(indat_path, DATAQUAL_distance_path+\"/distance.csv\", vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f7f64",
   "metadata": {},
   "source": [
    "## compute RMS_E2E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9ad77",
   "metadata": {},
   "source": [
    "doublechecked, works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16f14abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute RMS(E2E) for subject:  100\n",
      "Compute RMS(E2E) for subject:  106\n",
      "Compute RMS(E2E) for subject:  75\n",
      "Compute RMS(E2E) for subject:  87\n"
     ]
    }
   ],
   "source": [
    "computeRMSEyeToEye(indat_path, DATAQUAL_E2E_path+\"/E2E.csv\" , vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84796e",
   "metadata": {},
   "source": [
    "## S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0484d8d5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute RMS(S2S) for subject: 100\n",
      "Compute RMS(S2S) for subject: 106\n",
      "Compute RMS(S2S) for subject: 75\n",
      "Compute RMS(S2S) for subject: 87\n"
     ]
    }
   ],
   "source": [
    "computeRMSSampleToSample(indat_path, DATAQUAL_S2S_path+ \"/S2S.csv\", vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a29dc0",
   "metadata": {},
   "source": [
    "## compute trial-level data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a59e04",
   "metadata": {},
   "source": [
    "you will find the trial level data in the trialdat folder created earlier. <br>\n",
    "This will take a while, especially if you have a lot of data. Make a coffee in the meantime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb967c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute trial level data for subject: 100\n",
      "Compute trial level data for subject: 106\n",
      "Compute trial level data for subject: 75\n",
      "Compute trial level data for subject: 87\n"
     ]
    }
   ],
   "source": [
    "computeTrialLevelData(indat_path, trialdata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b0e08",
   "metadata": {},
   "source": [
    "todo: trial-level data validation, fixing saving file - not to write all files only in the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63a5a6",
   "metadata": {},
   "source": [
    "## extend trial level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f43fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extend trial level data with additional fields for subject: 100\n",
      "Extend trial level data with additional fields for subject: 106\n",
      "Extend trial level data with additional fields for subject: 75\n",
      "Extend trial level data with additional fields for subject: 87\n"
     ]
    }
   ],
   "source": [
    "extendTrialLevelData(trialdata_path, ext_trialdata_path, vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af82b7",
   "metadata": {},
   "source": [
    "# Chapter 2: creating trial-level datasets to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd94338",
   "metadata": {},
   "source": [
    "## cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "30e32e16",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0412d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## which cols do you want to keep?\n",
    "\n",
    "list_cols = ['subject_number', 'PCode', 'epoch', 'block', 'trial','triplet_type_hl', 'stimulus', 'RT',\n",
    "       'last_AOI_before_stimulus', 'repetition', 'trill', 'high_low_learning',\n",
    "       'has_anticipation', 'has_learnt_anticipation_OS', 'has_learnt_anticipation_IF']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c525a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dat_clean = clean_and_organize(list_cols, vars_dict, subs_with_bad_data, ext_trialdata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "622bcacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dat_clean.to_csv(output_path + \"/trialdat_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993205a9-53b4-4311-9107-896f38d8555a",
   "metadata": {},
   "source": [
    "## create an analyzable file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62cacb15-f327-426f-b048-001b3a792f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trialdat_HL_LMM= get_trialdat_ready_for_HL_LMM(raw_dat_clean, vars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "801fe7a5-0fc7-473a-8fcf-0c04ce3f7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trialdat_HL_LMM.to_csv(output_path + \"/trialdat_HL_LMM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474159a-2eb4-4efd-9bd7-f84010871114",
   "metadata": {},
   "source": [
    "## use this latter one to calculate wide HLdiff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eb71654a-9c93-45ce-b546-39780d5fbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1: create wide data\n",
    "wide_RT = simple_pivot(trialdat_HL_LMM, vars_dict[\"dep\"], vars_dict[\"ID\"], [vars_dict[\"epoch\"], vars_dict[\"HL_OS_var\"]], aggfunct = \"median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f666784b-3492-4e86-afad-f52844e11dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_number</th>\n",
       "      <th>1high</th>\n",
       "      <th>1low</th>\n",
       "      <th>2high</th>\n",
       "      <th>2low</th>\n",
       "      <th>3high</th>\n",
       "      <th>3low</th>\n",
       "      <th>4high</th>\n",
       "      <th>4low</th>\n",
       "      <th>5high</th>\n",
       "      <th>5low</th>\n",
       "      <th>6high</th>\n",
       "      <th>6low</th>\n",
       "      <th>7high</th>\n",
       "      <th>7low</th>\n",
       "      <th>8high</th>\n",
       "      <th>8low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>275.0405</td>\n",
       "      <td>274.9830</td>\n",
       "      <td>274.916</td>\n",
       "      <td>275.0930</td>\n",
       "      <td>275.0430</td>\n",
       "      <td>275.8910</td>\n",
       "      <td>283.2725</td>\n",
       "      <td>274.9970</td>\n",
       "      <td>266.7040</td>\n",
       "      <td>274.9300</td>\n",
       "      <td>274.895</td>\n",
       "      <td>266.774</td>\n",
       "      <td>275.065</td>\n",
       "      <td>274.9870</td>\n",
       "      <td>274.954</td>\n",
       "      <td>283.2870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>325.0680</td>\n",
       "      <td>308.3715</td>\n",
       "      <td>325.045</td>\n",
       "      <td>308.4420</td>\n",
       "      <td>333.3460</td>\n",
       "      <td>308.4020</td>\n",
       "      <td>341.6820</td>\n",
       "      <td>324.8930</td>\n",
       "      <td>333.3760</td>\n",
       "      <td>308.3985</td>\n",
       "      <td>308.477</td>\n",
       "      <td>308.391</td>\n",
       "      <td>308.499</td>\n",
       "      <td>316.7330</td>\n",
       "      <td>324.988</td>\n",
       "      <td>316.8840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>299.9580</td>\n",
       "      <td>291.6540</td>\n",
       "      <td>299.979</td>\n",
       "      <td>308.3835</td>\n",
       "      <td>304.1035</td>\n",
       "      <td>291.6770</td>\n",
       "      <td>283.3075</td>\n",
       "      <td>291.6870</td>\n",
       "      <td>291.8500</td>\n",
       "      <td>291.6830</td>\n",
       "      <td>283.430</td>\n",
       "      <td>291.606</td>\n",
       "      <td>291.724</td>\n",
       "      <td>291.6875</td>\n",
       "      <td>283.356</td>\n",
       "      <td>283.3255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>308.4280</td>\n",
       "      <td>324.9765</td>\n",
       "      <td>316.631</td>\n",
       "      <td>316.8870</td>\n",
       "      <td>308.3500</td>\n",
       "      <td>324.9565</td>\n",
       "      <td>308.3430</td>\n",
       "      <td>324.9065</td>\n",
       "      <td>312.3885</td>\n",
       "      <td>325.0330</td>\n",
       "      <td>324.933</td>\n",
       "      <td>324.970</td>\n",
       "      <td>315.991</td>\n",
       "      <td>333.3010</td>\n",
       "      <td>316.718</td>\n",
       "      <td>325.0570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_number     1high      1low    2high      2low     3high      3low  \\\n",
       "0              75  275.0405  274.9830  274.916  275.0930  275.0430  275.8910   \n",
       "1              87  325.0680  308.3715  325.045  308.4420  333.3460  308.4020   \n",
       "2             100  299.9580  291.6540  299.979  308.3835  304.1035  291.6770   \n",
       "3             106  308.4280  324.9765  316.631  316.8870  308.3500  324.9565   \n",
       "\n",
       "      4high      4low     5high      5low    6high     6low    7high  \\\n",
       "0  283.2725  274.9970  266.7040  274.9300  274.895  266.774  275.065   \n",
       "1  341.6820  324.8930  333.3760  308.3985  308.477  308.391  308.499   \n",
       "2  283.3075  291.6870  291.8500  291.6830  283.430  291.606  291.724   \n",
       "3  308.3430  324.9065  312.3885  325.0330  324.933  324.970  315.991   \n",
       "\n",
       "       7low    8high      8low  \n",
       "0  274.9870  274.954  283.2870  \n",
       "1  316.7330  324.988  316.8840  \n",
       "2  291.6875  283.356  283.3255  \n",
       "3  333.3010  316.718  325.0570  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_RT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0f6a5a5a-6be0-4a63-9a89-a6b6b3abfe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2: calc HLdiff\n",
    "wide_RT_HLdiff = calcHighLowDiff(wide_RT, vars_dict, is_RT = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7c80c45e-0225-4033-b3e6-83ff63b5e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LD anticipations\n",
    "\n",
    "#step1_ calc LDAEM\n",
    "wide_LDAEM = simple_pivot(trialdat_HL_LMM, \"has_learnt_anticipation_OS_int\", vars_dict[\"ID\"], vars_dict[\"epoch\"], aggfunct = \"mean\")\n",
    "#step2: calc AEM\n",
    "wide_AEM = simple_pivot(trialdat_HL_LMM, \"has_anticipation_int\", vars_dict[\"ID\"], vars_dict[\"epoch\"], aggfunct = \"mean\")\n",
    "#step3: calc their ratio\n",
    "wide_LDAR = wide_LDAEM/wide_AEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bd2f4285-ea1c-484d-a24d-bb4d9e9c9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_LDAR.to_csv(output_path + \"/wide_LDAR.csv\")\n",
    "wide_RT_HLdiff.to_csv(output_path + \"/wide_RT_HLdiff.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29594416-5415-4ecb-928c-1d247f1fb0a6",
   "metadata": {},
   "source": [
    "## creating HLLH interference measure col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "448e7e5d-bef9-44ee-b8b6-f10cb7a4dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_IF_epochs = process_IF_epochs(raw_dat_clean, vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777bfac",
   "metadata": {},
   "source": [
    "## compute response types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33971a58-482b-48e7-a938-ab6807f8d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_resp_all = response_data_calculator(raw_dat_clean, vars_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8bdd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_resp_all = drop_blockstarting_randoms(dat_resp_all, vars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9a913d32-26d0-4c7c-9a85-77345b7d7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_resp_all.to_csv(output_path + \"/dat_resp_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855ef21",
   "metadata": {},
   "source": [
    "## likelihood calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1c92a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_likelihood_wide = likelihood_generator(dat_resp_all, vars_dict, \"response_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6be7d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_likelihood_wide.to_csv(output_path + \"/resp_likelihood_wide.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd8c74",
   "metadata": {},
   "source": [
    "## update calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876e3a1",
   "metadata": {},
   "source": [
    "### update column (trial-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63f800b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_subject  updated to 100\n",
      "current_subject 100 updated to 106\n",
      "current_subject 106 updated to 75\n",
      "current_subject 75 updated to 87\n"
     ]
    }
   ],
   "source": [
    "dat_update_all = calculate_update_vars(dat_resp_all, vars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2409550c-1811-4e18-b0a6-ec6d0c35c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_update_all_firstdropped = dat_update_all.loc[dat_update_all[\"update_type\"] != \"first\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "84deb034-4d69-4c38-9dc4-de72617f1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_update_all_firstdropped.to_csv(\"dat_update_resptype_LMM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9178a",
   "metadata": {},
   "source": [
    "### update type likelihood calculation (wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d89dff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_likelihood_wide_dropped_first = likelihood_generator(dat_update_all_firstdropped, vars_dict, \"update_type\", probability_dict = {\"NLD_SAME\": 0.1875, \"LD_SAME\": 0.0625, \"EXPL\": 0.375, \"MOD_CORR\": 0.1875, \"MOD_DISR\": 0.1875})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4f5c555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_likelihood_wide_dropped_first.to_csv(output_path + \"/update_likelihood_wide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b58e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93cbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6bed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a785fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a130d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643279a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb0166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079db3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37883e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa7836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff5e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f283c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
